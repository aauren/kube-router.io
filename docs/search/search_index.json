{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Kube-router Documentation The documentation is divided into the following sections: Introduction What is Kube-router? Why Kube-router? Concepts See it in action How it works? Architecture User Guide installation requirements Operations guide Health Metrics Troubleshooting Pod toolbox Upgrades Developer and Contributor Guide developer guide Contributor Guideline","title":"Overview"},{"location":"#welcome-to-kube-router-documentation","text":"The documentation is divided into the following sections: Introduction What is Kube-router? Why Kube-router? Concepts See it in action How it works? Architecture User Guide installation requirements Operations guide Health Metrics Troubleshooting Pod toolbox Upgrades Developer and Contributor Guide developer guide Contributor Guideline","title":"Welcome to Kube-router Documentation"},{"location":"Observability/","text":"Observability Observing dropped traffic due to network policy enforcements Traffic that gets rejected due to network policy enforcements gets logged by kube-route using iptables NFLOG target under the group 100. Simplest way to observe the dropped packets by kube-router is by running tcpdump on nflog:100 interface for e.g. tcpdump -i nflog:100 -n . You can also configure ulogd to monitor dropped packets in desired output format. Please see https://kb.gtkc.net/iptables-with-ulogd-quick-howto/ for an example configuration to setup a stack to log packets.","title":"Observability"},{"location":"Observability/#observability","text":"","title":"Observability"},{"location":"Observability/#observing-dropped-traffic-due-to-network-policy-enforcements","text":"Traffic that gets rejected due to network policy enforcements gets logged by kube-route using iptables NFLOG target under the group 100. Simplest way to observe the dropped packets by kube-router is by running tcpdump on nflog:100 interface for e.g. tcpdump -i nflog:100 -n . You can also configure ulogd to monitor dropped packets in desired output format. Please see https://kb.gtkc.net/iptables-with-ulogd-quick-howto/ for an example configuration to setup a stack to log packets.","title":"Observing dropped traffic due to network policy enforcements"},{"location":"architecture/","text":"Architecture Kube-router is built around concept of watchers and controllers. Watchers use Kubernetes watch API to get notification on events related to create, update, delete of Kubernetes objects. Each watcher gets notification related to a particular API object. On receiving an event from API server, watcher broadcasts events. Controller registers to get event updates from the watchers and act up on the events. Kube-router consists of 3 core controllers and multiple watchers as depicted in below diagram. Each of the controller follows below structure func Run() { for { Sync() // control loop that runs for ever and perfom sync at periodic interval } } func OnUpdate() { Sync() // on receiving update of a watched API object (namespace, node, pod, network policy etc) } Sync() { //re-concile any state changes } Cleanup() { // cleanup any changes (to iptables, ipvs, network etc) done to the system }","title":"Architecture"},{"location":"architecture/#architecture","text":"Kube-router is built around concept of watchers and controllers. Watchers use Kubernetes watch API to get notification on events related to create, update, delete of Kubernetes objects. Each watcher gets notification related to a particular API object. On receiving an event from API server, watcher broadcasts events. Controller registers to get event updates from the watchers and act up on the events. Kube-router consists of 3 core controllers and multiple watchers as depicted in below diagram. Each of the controller follows below structure func Run() { for { Sync() // control loop that runs for ever and perfom sync at periodic interval } } func OnUpdate() { Sync() // on receiving update of a watched API object (namespace, node, pod, network policy etc) } Sync() { //re-concile any state changes } Cleanup() { // cleanup any changes (to iptables, ipvs, network etc) done to the system }","title":"Architecture"},{"location":"bgp/","text":"Configuring BGP Peers When kube-router is used to provide pod-to-pod networking, BGP is used to exchange routes across the nodes. Kube-router provides flexible networking models to support different deployments (public vs private cloud, routable vs non-routable pod IP's, service ip's etc). Peering Within The Cluster Full Node-To-Node Mesh This is the default mode. All nodes in the clusters form iBGP peering relationship with rest of the nodes forming full node-to-node mesh. Each node advertise the pod CIDR allocated to the nodes with peers (rest of the nodes in the cluster). There is no configuration required in this mode. All the nodes in the cluster are associated with private ASN 64512 implicitly (which can be configured with --cluster-asn flag). Users are transparent to use of iBGP. This mode is suitable in public cloud environments or small cluster deployments. Node-To-Node Peering Without Full Mesh This model support more than a single AS per cluster to allow AS per rack or AS per node models. Nodes in the cluster does not form full node-to-node mesh. Users has to explicitly select this mode by specifying --nodes-full-mesh=false when launching kube-router. In this mode kube-router expects each node is configured with an ASN number from the node's API object annonations. Kube-router will use the node's kube-router.io/node.asn annotation value as the ASN number for the node. Users can annotate node objects with the following command: kubectl annotate node <kube-node> \"kube-router.io/node.asn=64512\" Only nodes with in same ASN form full mesh. Two nodes with different ASNs never get peered. Route-Reflector setup Without Full Mesh This model support the common scheme of using Route Reflector Server node to concentrate peering from Client Peer. This has the big advantage of not needing full mesh, and scale better. In this mode kube-router expects each node is configured either in Route Reflector server mode or in Route Reflector client mode. This is done with node kube-router.io/rr.server=ClusterID , kube-router.io/rr.client=ClusterId respectively. In this mode each Route Reflector Client will only peer with Route Reflector Servers. Each Route Reflector Server will peer other Route Reflector Server and with Route Reflector Clients enabling reflection. Users can annotate node objects with the following command: kubectl annotate node <kube-node> \"kube-router.io/rr.server=42\" for Route Reflector server mode, and kubectl annotate node <kube-node> \"kube-router.io/rr.client=42\" for Route Reflector client mode. Only nodes with the same ClusterID in client and server mode will peer together. When joining new nodes to the cluster, remember to annotate them with kube-router.io/rr.client=42 , and then restart kube-router on the new nodes and the route reflector server nodes to let them successfully read the annotations and peer with each other. Peering Outside The Cluster Global External BGP Peers An optional global BGP peer can be configured by specifying --peer-router-asns and --peer-router-ips parameters. When configured each node in the cluster forms a peer relationship with specified global peer. Pod CIDR and Cluster IP's get advertised to the global BGP peer. For redundancy you can also configure more than one peer router by specifying a slice of BGP peers. For example: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=65000,65000 Node Specific External BGP Peers Alternatively, each node can be configured with one or more node specific BGP peers. Information regarding node specific BGP peer is read from node API object annotations: - kube-router.io/peer.ips - kube-router.io/peer.asns For e.g users can annotate node object with below commands kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\" AS Path Prepending For traffic shaping purposes, you may want to prepend the AS path announced to peers. This can be accomplished on a per-node basis with annotations: - kube-router.io/path-prepend.as - kube-router.io/path-prepend.repeat-n If you wanted to prepend all routes from a particular node with the AS 65000 five times, you would run the following commands: kubectl annotate node <kube-node> \"kube-router.io/path-prepend.as=65000\" kubectl annotate node <kube-node> \"kube-router.io/path-prepend.repeat-n=5\" BGP Peer Password Authentication The examples above have assumed there is no password authentication with BGP peer routers. If you need to use a password for peering, you can use the --peer-router-passwords command-line option, the kube-router.io/peer.passwords node annotation, or the --peer-router-passwords-file command-line option. Base64 Encoding Passwords To ensure passwords are easily parsed, but not easily read by human eyes, kube-router requires that they are encoded as base64. On a Linux or MacOS system you can encode your passwords on the command line: $ printf \"SecurePassword\" | base64 U2VjdXJlUGFzc3dvcmQ= Password Configuration Examples In this CLI flag example the first router (192.168.1.99) uses a password, while the second (192.168.1.100) does not. --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords=\"U2VjdXJlUGFzc3dvcmQK,\" Note the comma indicating the end of the first password. Now here's the same example but configured as node annotations: kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\" kubectl annotate node <kube-node> \"kube-router.io/peer.passwords=U2VjdXJlUGFzc3dvcmQK,\" Finally, to include peer passwords as a file you would run kube-router with the following option: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords-file=\"/etc/kube-router/bgp-passwords.conf\" The password file, closely follows the syntax of the command-line and node annotation options. Here, the first peer IP (192.168.1.99) would be configured with a password, while the second would not. U2VjdXJlUGFzc3dvcmQK, Note, complex parsing is not done on this file, please do not include any content other than the passwords on a single line in this file. BGP Communities Global peers support the addition of BGP communities via node annotations. Node annotations can be formulated either as: * a single 32-bit integer * two 16-bit integers separated by a colon ( : ) * common BGP community names (e.g. no-export , internet , no-peer , etc.) (see: WellKnownCommunityNameMap ) In the following example we add the NO_EXPORT BGP community to two of our nodes via annotation using all three forms of the annotation: kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=4294967041\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=65535:65281\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=no-export\" BGP listen address list By default, GoBGP server binds on the node IP address. However in case of nodes with multiple IP address it is desirable to bind GoBGP to multiple local adresses. Local IP address on which GoGBP should listen on a node can be configured with annotation kube-router.io/bgp-local-addresses . Here is sample example to make GoBGP server to listen on multiple IP address kubectl annotate node ip-172-20-46-87.us-west-2.compute.internal \"kube-router.io/bgp-local-addresses=172.20.56.25,192.168.1.99\" Overriding the next hop By default kube-router populates GoBGP RIB with node IP as next hop for the advertised pod CIDR's and service VIP. While this works for most cases, overriding the next hop for the advertised rotues is necessary when node has multiple interfaces over which external peers are reached. Next hop need to be as per the interface local IP over which external peer can be reached. --override-nexthop let you override the next hop for the advertised route. Setting --override-nexthop to true leverages BGP next-hop-self functionality implemented in GoBGP. Next hop will automatically selected appropriately when advertising routes irrespective of the next hop in the RIB. Overriding the next hop and enable IPIP/tuennel A common scenario exists where each node in the cluster is connected to two upstream routers that are in two different subnets. For example, one router is connected to a public network subnet and the other router is connected to a private network subnet. Additionally, nodes may be split across different subnets (e.g. different racks) each of which has their own routers. In this scenario, --override-nexthop can be used to correctly peer with each upstream router, ensuring that the BGP next-hop attribute is correctly set to the node's IP address that faces the upstream router. The --enable-overlay option can be set to allow overlay/underlay tunneling across the different subnets to achieve an interconnected pod network. This configuration would have the following effects: Peering Outside the Cluster (https://github.com/cloudnativelabs/kube-router/blob/master/docs/bgp.md#peering-outside-the-cluster) via one of the many means that kube-router makes that option available Overriding Next Hop Enabling overlays in either full mode or with nodes in different subnets The warning here is that when using --override-nexthop in the above scenario, it may cause kube-router to advertise an IP address other than the node IP which is what kube-router connects the tunnel to when the --enable-overlay option is given. If this happens it may cause some network flows to become un-routable. Specifically, people need to take care when combining --override-nexthop and --enable-overlay and make sure that they understand their network, the flows they desire, how the kube-router logic works, and the possible side-effects that are created from their configuration. Please refer to this PR for the risk and impact discussion https://github.com/cloudnativelabs/kube-router/pull/1025.","title":"BGP"},{"location":"bgp/#configuring-bgp-peers","text":"When kube-router is used to provide pod-to-pod networking, BGP is used to exchange routes across the nodes. Kube-router provides flexible networking models to support different deployments (public vs private cloud, routable vs non-routable pod IP's, service ip's etc).","title":"Configuring BGP Peers"},{"location":"bgp/#peering-within-the-cluster","text":"","title":"Peering Within The Cluster"},{"location":"bgp/#full-node-to-node-mesh","text":"This is the default mode. All nodes in the clusters form iBGP peering relationship with rest of the nodes forming full node-to-node mesh. Each node advertise the pod CIDR allocated to the nodes with peers (rest of the nodes in the cluster). There is no configuration required in this mode. All the nodes in the cluster are associated with private ASN 64512 implicitly (which can be configured with --cluster-asn flag). Users are transparent to use of iBGP. This mode is suitable in public cloud environments or small cluster deployments.","title":"Full Node-To-Node Mesh"},{"location":"bgp/#node-to-node-peering-without-full-mesh","text":"This model support more than a single AS per cluster to allow AS per rack or AS per node models. Nodes in the cluster does not form full node-to-node mesh. Users has to explicitly select this mode by specifying --nodes-full-mesh=false when launching kube-router. In this mode kube-router expects each node is configured with an ASN number from the node's API object annonations. Kube-router will use the node's kube-router.io/node.asn annotation value as the ASN number for the node. Users can annotate node objects with the following command: kubectl annotate node <kube-node> \"kube-router.io/node.asn=64512\" Only nodes with in same ASN form full mesh. Two nodes with different ASNs never get peered.","title":"Node-To-Node Peering Without Full Mesh"},{"location":"bgp/#route-reflector-setup-without-full-mesh","text":"This model support the common scheme of using Route Reflector Server node to concentrate peering from Client Peer. This has the big advantage of not needing full mesh, and scale better. In this mode kube-router expects each node is configured either in Route Reflector server mode or in Route Reflector client mode. This is done with node kube-router.io/rr.server=ClusterID , kube-router.io/rr.client=ClusterId respectively. In this mode each Route Reflector Client will only peer with Route Reflector Servers. Each Route Reflector Server will peer other Route Reflector Server and with Route Reflector Clients enabling reflection. Users can annotate node objects with the following command: kubectl annotate node <kube-node> \"kube-router.io/rr.server=42\" for Route Reflector server mode, and kubectl annotate node <kube-node> \"kube-router.io/rr.client=42\" for Route Reflector client mode. Only nodes with the same ClusterID in client and server mode will peer together. When joining new nodes to the cluster, remember to annotate them with kube-router.io/rr.client=42 , and then restart kube-router on the new nodes and the route reflector server nodes to let them successfully read the annotations and peer with each other.","title":"Route-Reflector setup  Without Full Mesh"},{"location":"bgp/#peering-outside-the-cluster","text":"","title":"Peering Outside The Cluster"},{"location":"bgp/#global-external-bgp-peers","text":"An optional global BGP peer can be configured by specifying --peer-router-asns and --peer-router-ips parameters. When configured each node in the cluster forms a peer relationship with specified global peer. Pod CIDR and Cluster IP's get advertised to the global BGP peer. For redundancy you can also configure more than one peer router by specifying a slice of BGP peers. For example: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=65000,65000","title":"Global External BGP Peers"},{"location":"bgp/#node-specific-external-bgp-peers","text":"Alternatively, each node can be configured with one or more node specific BGP peers. Information regarding node specific BGP peer is read from node API object annotations: - kube-router.io/peer.ips - kube-router.io/peer.asns For e.g users can annotate node object with below commands kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\"","title":"Node Specific External BGP Peers"},{"location":"bgp/#as-path-prepending","text":"For traffic shaping purposes, you may want to prepend the AS path announced to peers. This can be accomplished on a per-node basis with annotations: - kube-router.io/path-prepend.as - kube-router.io/path-prepend.repeat-n If you wanted to prepend all routes from a particular node with the AS 65000 five times, you would run the following commands: kubectl annotate node <kube-node> \"kube-router.io/path-prepend.as=65000\" kubectl annotate node <kube-node> \"kube-router.io/path-prepend.repeat-n=5\"","title":"AS Path Prepending"},{"location":"bgp/#bgp-peer-password-authentication","text":"The examples above have assumed there is no password authentication with BGP peer routers. If you need to use a password for peering, you can use the --peer-router-passwords command-line option, the kube-router.io/peer.passwords node annotation, or the --peer-router-passwords-file command-line option.","title":"BGP Peer Password Authentication"},{"location":"bgp/#base64-encoding-passwords","text":"To ensure passwords are easily parsed, but not easily read by human eyes, kube-router requires that they are encoded as base64. On a Linux or MacOS system you can encode your passwords on the command line: $ printf \"SecurePassword\" | base64 U2VjdXJlUGFzc3dvcmQ=","title":"Base64 Encoding Passwords"},{"location":"bgp/#password-configuration-examples","text":"In this CLI flag example the first router (192.168.1.99) uses a password, while the second (192.168.1.100) does not. --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords=\"U2VjdXJlUGFzc3dvcmQK,\" Note the comma indicating the end of the first password. Now here's the same example but configured as node annotations: kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\" kubectl annotate node <kube-node> \"kube-router.io/peer.passwords=U2VjdXJlUGFzc3dvcmQK,\" Finally, to include peer passwords as a file you would run kube-router with the following option: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords-file=\"/etc/kube-router/bgp-passwords.conf\" The password file, closely follows the syntax of the command-line and node annotation options. Here, the first peer IP (192.168.1.99) would be configured with a password, while the second would not. U2VjdXJlUGFzc3dvcmQK, Note, complex parsing is not done on this file, please do not include any content other than the passwords on a single line in this file.","title":"Password Configuration Examples"},{"location":"bgp/#bgp-communities","text":"Global peers support the addition of BGP communities via node annotations. Node annotations can be formulated either as: * a single 32-bit integer * two 16-bit integers separated by a colon ( : ) * common BGP community names (e.g. no-export , internet , no-peer , etc.) (see: WellKnownCommunityNameMap ) In the following example we add the NO_EXPORT BGP community to two of our nodes via annotation using all three forms of the annotation: kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=4294967041\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=65535:65281\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=no-export\"","title":"BGP Communities"},{"location":"bgp/#bgp-listen-address-list","text":"By default, GoBGP server binds on the node IP address. However in case of nodes with multiple IP address it is desirable to bind GoBGP to multiple local adresses. Local IP address on which GoGBP should listen on a node can be configured with annotation kube-router.io/bgp-local-addresses . Here is sample example to make GoBGP server to listen on multiple IP address kubectl annotate node ip-172-20-46-87.us-west-2.compute.internal \"kube-router.io/bgp-local-addresses=172.20.56.25,192.168.1.99\"","title":"BGP listen address list"},{"location":"bgp/#overriding-the-next-hop","text":"By default kube-router populates GoBGP RIB with node IP as next hop for the advertised pod CIDR's and service VIP. While this works for most cases, overriding the next hop for the advertised rotues is necessary when node has multiple interfaces over which external peers are reached. Next hop need to be as per the interface local IP over which external peer can be reached. --override-nexthop let you override the next hop for the advertised route. Setting --override-nexthop to true leverages BGP next-hop-self functionality implemented in GoBGP. Next hop will automatically selected appropriately when advertising routes irrespective of the next hop in the RIB.","title":"Overriding the next hop"},{"location":"bgp/#overriding-the-next-hop-and-enable-ipiptuennel","text":"A common scenario exists where each node in the cluster is connected to two upstream routers that are in two different subnets. For example, one router is connected to a public network subnet and the other router is connected to a private network subnet. Additionally, nodes may be split across different subnets (e.g. different racks) each of which has their own routers. In this scenario, --override-nexthop can be used to correctly peer with each upstream router, ensuring that the BGP next-hop attribute is correctly set to the node's IP address that faces the upstream router. The --enable-overlay option can be set to allow overlay/underlay tunneling across the different subnets to achieve an interconnected pod network. This configuration would have the following effects: Peering Outside the Cluster (https://github.com/cloudnativelabs/kube-router/blob/master/docs/bgp.md#peering-outside-the-cluster) via one of the many means that kube-router makes that option available Overriding Next Hop Enabling overlays in either full mode or with nodes in different subnets The warning here is that when using --override-nexthop in the above scenario, it may cause kube-router to advertise an IP address other than the node IP which is what kube-router connects the tunnel to when the --enable-overlay option is given. If this happens it may cause some network flows to become un-routable. Specifically, people need to take care when combining --override-nexthop and --enable-overlay and make sure that they understand their network, the flows they desire, how the kube-router logic works, and the possible side-effects that are created from their configuration. Please refer to this PR for the risk and impact discussion https://github.com/cloudnativelabs/kube-router/pull/1025.","title":"Overriding the next hop and enable IPIP/tuennel"},{"location":"bootkube/","text":"Bootkube Integration The following instructions and examples demonstrate how to create a new Bootkube provisioned Kubernetes cluster using kube-router in place of kube-proxy and flannel. Asset Creation Follow the documentation for your environment and setup arguments for bootkube render . For example: bootkube render --asset-dir=${PWD}/assets --api-servers=https://kube-api-dev.zbrbdl:443 --api-server-alt-names=DNS=kube-api-dev.zbrbdl --etcd-servers=\"http://127.0.0.1:2379\" Writing asset: /home/bzub/assets/manifests/kube-scheduler.yaml Writing asset: /home/bzub/assets/manifests/kube-scheduler-disruption.yaml Writing asset: /home/bzub/assets/manifests/kube-controller-manager-disruption.yaml Writing asset: /home/bzub/assets/manifests/kube-dns-deployment.yaml Writing asset: /home/bzub/assets/manifests/pod-checkpointer.yaml Writing asset: /home/bzub/assets/manifests/kube-flannel.yaml Writing asset: /home/bzub/assets/manifests/kube-system-rbac-role-binding.yaml Writing asset: /home/bzub/assets/manifests/kube-controller-manager.yaml Writing asset: /home/bzub/assets/manifests/kube-apiserver.yaml Writing asset: /home/bzub/assets/manifests/kube-proxy.yaml Writing asset: /home/bzub/assets/manifests/kube-flannel-cfg.yaml Writing asset: /home/bzub/assets/manifests/kube-dns-svc.yaml Writing asset: /home/bzub/assets/bootstrap-manifests/bootstrap-apiserver.yaml Writing asset: /home/bzub/assets/bootstrap-manifests/bootstrap-controller-manager.yaml Writing asset: /home/bzub/assets/bootstrap-manifests/bootstrap-scheduler.yaml Writing asset: /home/bzub/assets/tls/ca.key Writing asset: /home/bzub/assets/tls/ca.crt Writing asset: /home/bzub/assets/tls/apiserver.key Writing asset: /home/bzub/assets/tls/apiserver.crt Writing asset: /home/bzub/assets/tls/service-account.key Writing asset: /home/bzub/assets/tls/service-account.pub Writing asset: /home/bzub/assets/tls/kubelet.key Writing asset: /home/bzub/assets/tls/kubelet.crt Writing asset: /home/bzub/assets/auth/kubeconfig Writing asset: /home/bzub/assets/manifests/kube-apiserver-secret.yaml Writing asset: /home/bzub/assets/manifests/kube-controller-manager-secret.yaml Kube-router Installation Next move/delete the manifests for kube-proxy and flannel from assets/manifests and replace them with the kube-router.yaml and kube-router-cfg.yaml files provided in this repo. rm assets/manifests/kube-flannel{,-cfg}.yaml assets/manifests/kube-proxy.yaml curl -L https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/contrib/bootkube/kube-router-cfg.yaml -o assets/manifests/kube-router-cfg.yaml curl -L https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/contrib/bootkube/kube-router.yaml -o assets/manifests/kube-router.yaml Cluster Startup Finally, proceed by following the Bootkube documentation, which generally involves starting Kubelet and running bootkube start referring to your assets directory on a new Kubernetes node. After starting multiple master nodes, our example cluster looks like this: $ kubectl -n kube-system get pods,services NAME READY STATUS RESTARTS AGE IP NODE po/kube-apiserver-gztjp 1/1 Running 0 15h 10.10.3.2 node2-dev.zbrbdl po/kube-apiserver-h55t7 1/1 Running 0 15h 10.10.3.3 node3-dev.zbrbdl po/kube-apiserver-qn5xm 1/1 Running 2 15h 10.10.3.1 node1-dev.zbrbdl po/kube-controller-manager-3052101514-kp121 1/1 Running 1 15h 10.2.0.5 node1-dev.zbrbdl po/kube-controller-manager-3052101514-n4q9p 1/1 Running 2 15h 10.2.0.6 node1-dev.zbrbdl po/kube-dns-2431531914-pr9lg 3/3 Running 0 15h 10.2.0.3 node1-dev.zbrbdl po/kube-router-ckdj1 1/1 Running 15 15h 10.10.3.3 node3-dev.zbrbdl po/kube-router-dcgbr 1/1 Running 15 15h 10.10.3.1 node1-dev.zbrbdl po/kube-router-n0vcn 1/1 Running 15 15h 10.10.3.2 node2-dev.zbrbdl po/kube-scheduler-2172662190-g4q3w 1/1 Running 4 15h 10.2.0.2 node1-dev.zbrbdl po/kube-scheduler-2172662190-hcq3t 1/1 Running 2 15h 10.2.0.4 node1-dev.zbrbdl po/pod-checkpointer-jlfsv 1/1 Running 0 15h 10.10.3.1 node1-dev.zbrbdl po/pod-checkpointer-jlfsv-node1-dev.zbrbdl 1/1 Running 0 15h 10.10.3.1 node1-dev.zbrbdl po/pod-checkpointer-lhckt 1/1 Running 0 15h 10.10.3.3 node3-dev.zbrbdl po/pod-checkpointer-lhckt-node3-dev.zbrbdl 1/1 Running 0 15h 10.10.3.3 node3-dev.zbrbdl po/pod-checkpointer-tsbkh 1/1 Running 0 15h 10.10.3.2 node2-dev.zbrbdl po/pod-checkpointer-tsbkh-node2-dev.zbrbdl 1/1 Running 0 15h 10.10.3.2 node2-dev.zbrbdl NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc/kube-dns 10.3.0.10 <none> 53/UDP,53/TCP 15h k8s-app=kube-dns","title":"Bootkube Integration"},{"location":"bootkube/#bootkube-integration","text":"The following instructions and examples demonstrate how to create a new Bootkube provisioned Kubernetes cluster using kube-router in place of kube-proxy and flannel.","title":"Bootkube Integration"},{"location":"bootkube/#asset-creation","text":"Follow the documentation for your environment and setup arguments for bootkube render . For example: bootkube render --asset-dir=${PWD}/assets --api-servers=https://kube-api-dev.zbrbdl:443 --api-server-alt-names=DNS=kube-api-dev.zbrbdl --etcd-servers=\"http://127.0.0.1:2379\" Writing asset: /home/bzub/assets/manifests/kube-scheduler.yaml Writing asset: /home/bzub/assets/manifests/kube-scheduler-disruption.yaml Writing asset: /home/bzub/assets/manifests/kube-controller-manager-disruption.yaml Writing asset: /home/bzub/assets/manifests/kube-dns-deployment.yaml Writing asset: /home/bzub/assets/manifests/pod-checkpointer.yaml Writing asset: /home/bzub/assets/manifests/kube-flannel.yaml Writing asset: /home/bzub/assets/manifests/kube-system-rbac-role-binding.yaml Writing asset: /home/bzub/assets/manifests/kube-controller-manager.yaml Writing asset: /home/bzub/assets/manifests/kube-apiserver.yaml Writing asset: /home/bzub/assets/manifests/kube-proxy.yaml Writing asset: /home/bzub/assets/manifests/kube-flannel-cfg.yaml Writing asset: /home/bzub/assets/manifests/kube-dns-svc.yaml Writing asset: /home/bzub/assets/bootstrap-manifests/bootstrap-apiserver.yaml Writing asset: /home/bzub/assets/bootstrap-manifests/bootstrap-controller-manager.yaml Writing asset: /home/bzub/assets/bootstrap-manifests/bootstrap-scheduler.yaml Writing asset: /home/bzub/assets/tls/ca.key Writing asset: /home/bzub/assets/tls/ca.crt Writing asset: /home/bzub/assets/tls/apiserver.key Writing asset: /home/bzub/assets/tls/apiserver.crt Writing asset: /home/bzub/assets/tls/service-account.key Writing asset: /home/bzub/assets/tls/service-account.pub Writing asset: /home/bzub/assets/tls/kubelet.key Writing asset: /home/bzub/assets/tls/kubelet.crt Writing asset: /home/bzub/assets/auth/kubeconfig Writing asset: /home/bzub/assets/manifests/kube-apiserver-secret.yaml Writing asset: /home/bzub/assets/manifests/kube-controller-manager-secret.yaml","title":"Asset Creation"},{"location":"bootkube/#kube-router-installation","text":"Next move/delete the manifests for kube-proxy and flannel from assets/manifests and replace them with the kube-router.yaml and kube-router-cfg.yaml files provided in this repo. rm assets/manifests/kube-flannel{,-cfg}.yaml assets/manifests/kube-proxy.yaml curl -L https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/contrib/bootkube/kube-router-cfg.yaml -o assets/manifests/kube-router-cfg.yaml curl -L https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/contrib/bootkube/kube-router.yaml -o assets/manifests/kube-router.yaml","title":"Kube-router Installation"},{"location":"bootkube/#cluster-startup","text":"Finally, proceed by following the Bootkube documentation, which generally involves starting Kubelet and running bootkube start referring to your assets directory on a new Kubernetes node. After starting multiple master nodes, our example cluster looks like this: $ kubectl -n kube-system get pods,services NAME READY STATUS RESTARTS AGE IP NODE po/kube-apiserver-gztjp 1/1 Running 0 15h 10.10.3.2 node2-dev.zbrbdl po/kube-apiserver-h55t7 1/1 Running 0 15h 10.10.3.3 node3-dev.zbrbdl po/kube-apiserver-qn5xm 1/1 Running 2 15h 10.10.3.1 node1-dev.zbrbdl po/kube-controller-manager-3052101514-kp121 1/1 Running 1 15h 10.2.0.5 node1-dev.zbrbdl po/kube-controller-manager-3052101514-n4q9p 1/1 Running 2 15h 10.2.0.6 node1-dev.zbrbdl po/kube-dns-2431531914-pr9lg 3/3 Running 0 15h 10.2.0.3 node1-dev.zbrbdl po/kube-router-ckdj1 1/1 Running 15 15h 10.10.3.3 node3-dev.zbrbdl po/kube-router-dcgbr 1/1 Running 15 15h 10.10.3.1 node1-dev.zbrbdl po/kube-router-n0vcn 1/1 Running 15 15h 10.10.3.2 node2-dev.zbrbdl po/kube-scheduler-2172662190-g4q3w 1/1 Running 4 15h 10.2.0.2 node1-dev.zbrbdl po/kube-scheduler-2172662190-hcq3t 1/1 Running 2 15h 10.2.0.4 node1-dev.zbrbdl po/pod-checkpointer-jlfsv 1/1 Running 0 15h 10.10.3.1 node1-dev.zbrbdl po/pod-checkpointer-jlfsv-node1-dev.zbrbdl 1/1 Running 0 15h 10.10.3.1 node1-dev.zbrbdl po/pod-checkpointer-lhckt 1/1 Running 0 15h 10.10.3.3 node3-dev.zbrbdl po/pod-checkpointer-lhckt-node3-dev.zbrbdl 1/1 Running 0 15h 10.10.3.3 node3-dev.zbrbdl po/pod-checkpointer-tsbkh 1/1 Running 0 15h 10.10.3.2 node2-dev.zbrbdl po/pod-checkpointer-tsbkh-node2-dev.zbrbdl 1/1 Running 0 15h 10.10.3.2 node2-dev.zbrbdl NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc/kube-dns 10.3.0.10 <none> 53/UDP,53/TCP 15h k8s-app=kube-dns","title":"Cluster Startup"},{"location":"developing/","text":"Developer's Guide We aim to make local development and testing as straightforward as possible. For basic guidelines around contributing, see the CONTRIBUTING document. There are a number of automation tools available to help with testing and building your changes, detailed below. Building kube-router Go version 1.13 or above is required to build kube-router All the dependencies are specified as Go modules and will be fetched into your cache, so just run make or go build -o kube-router kube-router.go to build. Building A Docker Image Running make container will compile kube-router (if needed) and build a Docker image. By default the container will be tagged with the last release version, and current commit ID. For example: $ make container docker build -t \"cloudnativelabs/kube-router-git:0.0.4-22-gd782e89-dirty-build-release\" Sending build context to Docker daemon 151.5MB Step 1/4 : FROM alpine ---> a41a7446062d Step 2/4 : RUN apk add --no-cache iptables ipset ---> Using cache ---> 30e25a7640de Step 3/4 : COPY kube-router / ---> Using cache ---> c06f78fd02e8 Step 4/4 : ENTRYPOINT /kube-router ---> Using cache ---> 5cfcfe54623e Successfully built 5cfcfe54623e Successfully tagged cloudnativelabs/kube-router-git:0.0.4-22-gd782e89-dirty-build-release The -dirty part of the tag means there are uncommitted changes in your local git repo. Pushing A Docker Image Running make push will push your container image to a Docker registry. The default configuration will use the Docker Hub repository for the official kube-router images, cloudnativelabs/kube-router. You can push to a different repository by changing a couple settings, as described in Image Options below. Makefile Options There are several variables which can be modified in the Makefile to customize your builds. They are specified after your make command like this: make OPTION=VALUE . These options can also be set in your environment variables. For more details beyond the scope of this document, see the Makefile and run make help . Image Options You can configure the name and tag of the Docker image with a few variables passed to make container and make push . Example: $ make container IMG_FQDN=quay.io IMG_NAMESPACE=bzub IMG_TAG=custom docker build -t \"quay.io/bzub/kube-router-git:custom\" . Sending build context to Docker daemon 151.5MB Step 1/4 : FROM alpine ---> a41a7446062d Step 2/4 : RUN apk add --no-cache iptables ipset ---> Using cache ---> 30e25a7640de Step 3/4 : COPY kube-router / ---> Using cache ---> c06f78fd02e8 Step 4/4 : ENTRYPOINT /kube-router ---> Using cache ---> 5cfcfe54623e Successfully built 5cfcfe54623e Successfully tagged quay.io/bzub/kube-router-git:custom REGISTRY is derived from other options. Set this to something else to quickly override the Docker image registry used to tag and push images. Note: This will override other variables below that make up the image name/tag. IMG_FQDN should be set if you are not using Docker Hub for images. In the examples above IMG_FQDN is set to quay.io . IMG_NAMESPACE is the Docker registry user or organization. It is used in URLs. Example: quay.io/IMG_NAMESPACE/kube-router NAME goes onto the end of the Docker registry URL that will be used. Example: quay.io/cloudnativelabs/NAME IMG_TAG is used to override the tag of the Docker image being built. DEV_SUFFIX is appended to Docker image names that are not for release. By default these images get a name ending with -git to signify that they are for testing purposes. Example (DEV-SUFFIX=master-latest): quay.io/cloudnativelabs/kube-router-git:master-latest Release Workflow These instructions show how official kube-router releases are performed. First, you must tag a git commit with the release version. This will cause the CI system to: - Build kube-router - Build a Docker image with ${VERSION} and latest tags - Push the Docker image to the official registry - Submits a draft release to GitHub Example: VERSION=v0.5.0 git tag -a ${VERSION} -m \"Brief release note\" && git push origin ${VERSION} Then the only thing left to do is edit the release notes on the GitHub release and publish it. Manual Releases These instructions show how To perform a custom or test release outside of the CI system, using a local git commit. First tag a commit: VERSION=v0.5.0_bzub git tag -a ${VERSION} -m \"Brief release note\" Then you can provide options to make release . This does the following: - Builds kube-router - Builds a Docker image - Tags the image with the current git commit's tag - Tags the image with latest - Pushes the image to a docker registry If you'd like to test the GitHub release functionality as well, you will need to pass in the GITHUB_TOKEN variable with a value of an API token you've generated . This Access Token must have the \"repo\" OAuth scope enabled. NOTE: For added security when running a command that contains secure credentials, add a space before the entire command to prevent it from being added to your shell history file. Example: $ make release IMG_FQDN=quay.io IMG_NAMESPACE=bzub GITHUB_TOKEN=b1ahbl1ahb1ahba1hahb1ah Dependency Management kube-router uses dep for managing dependencies. Instructions on installing and using dep can be found here .","title":"Developing"},{"location":"developing/#developers-guide","text":"We aim to make local development and testing as straightforward as possible. For basic guidelines around contributing, see the CONTRIBUTING document. There are a number of automation tools available to help with testing and building your changes, detailed below.","title":"Developer's Guide"},{"location":"developing/#building-kube-router","text":"Go version 1.13 or above is required to build kube-router All the dependencies are specified as Go modules and will be fetched into your cache, so just run make or go build -o kube-router kube-router.go to build.","title":"Building kube-router"},{"location":"developing/#building-a-docker-image","text":"Running make container will compile kube-router (if needed) and build a Docker image. By default the container will be tagged with the last release version, and current commit ID. For example: $ make container docker build -t \"cloudnativelabs/kube-router-git:0.0.4-22-gd782e89-dirty-build-release\" Sending build context to Docker daemon 151.5MB Step 1/4 : FROM alpine ---> a41a7446062d Step 2/4 : RUN apk add --no-cache iptables ipset ---> Using cache ---> 30e25a7640de Step 3/4 : COPY kube-router / ---> Using cache ---> c06f78fd02e8 Step 4/4 : ENTRYPOINT /kube-router ---> Using cache ---> 5cfcfe54623e Successfully built 5cfcfe54623e Successfully tagged cloudnativelabs/kube-router-git:0.0.4-22-gd782e89-dirty-build-release The -dirty part of the tag means there are uncommitted changes in your local git repo.","title":"Building A Docker Image"},{"location":"developing/#pushing-a-docker-image","text":"Running make push will push your container image to a Docker registry. The default configuration will use the Docker Hub repository for the official kube-router images, cloudnativelabs/kube-router. You can push to a different repository by changing a couple settings, as described in Image Options below.","title":"Pushing A Docker Image"},{"location":"developing/#makefile-options","text":"There are several variables which can be modified in the Makefile to customize your builds. They are specified after your make command like this: make OPTION=VALUE . These options can also be set in your environment variables. For more details beyond the scope of this document, see the Makefile and run make help .","title":"Makefile Options"},{"location":"developing/#image-options","text":"You can configure the name and tag of the Docker image with a few variables passed to make container and make push . Example: $ make container IMG_FQDN=quay.io IMG_NAMESPACE=bzub IMG_TAG=custom docker build -t \"quay.io/bzub/kube-router-git:custom\" . Sending build context to Docker daemon 151.5MB Step 1/4 : FROM alpine ---> a41a7446062d Step 2/4 : RUN apk add --no-cache iptables ipset ---> Using cache ---> 30e25a7640de Step 3/4 : COPY kube-router / ---> Using cache ---> c06f78fd02e8 Step 4/4 : ENTRYPOINT /kube-router ---> Using cache ---> 5cfcfe54623e Successfully built 5cfcfe54623e Successfully tagged quay.io/bzub/kube-router-git:custom REGISTRY is derived from other options. Set this to something else to quickly override the Docker image registry used to tag and push images. Note: This will override other variables below that make up the image name/tag. IMG_FQDN should be set if you are not using Docker Hub for images. In the examples above IMG_FQDN is set to quay.io . IMG_NAMESPACE is the Docker registry user or organization. It is used in URLs. Example: quay.io/IMG_NAMESPACE/kube-router NAME goes onto the end of the Docker registry URL that will be used. Example: quay.io/cloudnativelabs/NAME IMG_TAG is used to override the tag of the Docker image being built. DEV_SUFFIX is appended to Docker image names that are not for release. By default these images get a name ending with -git to signify that they are for testing purposes. Example (DEV-SUFFIX=master-latest): quay.io/cloudnativelabs/kube-router-git:master-latest","title":"Image Options"},{"location":"developing/#release-workflow","text":"These instructions show how official kube-router releases are performed. First, you must tag a git commit with the release version. This will cause the CI system to: - Build kube-router - Build a Docker image with ${VERSION} and latest tags - Push the Docker image to the official registry - Submits a draft release to GitHub Example: VERSION=v0.5.0 git tag -a ${VERSION} -m \"Brief release note\" && git push origin ${VERSION} Then the only thing left to do is edit the release notes on the GitHub release and publish it.","title":"Release Workflow"},{"location":"developing/#manual-releases","text":"These instructions show how To perform a custom or test release outside of the CI system, using a local git commit. First tag a commit: VERSION=v0.5.0_bzub git tag -a ${VERSION} -m \"Brief release note\" Then you can provide options to make release . This does the following: - Builds kube-router - Builds a Docker image - Tags the image with the current git commit's tag - Tags the image with latest - Pushes the image to a docker registry If you'd like to test the GitHub release functionality as well, you will need to pass in the GITHUB_TOKEN variable with a value of an API token you've generated . This Access Token must have the \"repo\" OAuth scope enabled. NOTE: For added security when running a command that contains secure credentials, add a space before the entire command to prevent it from being added to your shell history file. Example: $ make release IMG_FQDN=quay.io IMG_NAMESPACE=bzub GITHUB_TOKEN=b1ahbl1ahb1ahba1hahb1ah","title":"Manual Releases"},{"location":"developing/#dependency-management","text":"kube-router uses dep for managing dependencies. Instructions on installing and using dep can be found here .","title":"Dependency Management"},{"location":"dsr/","text":"Direct Server Return More Information For a more detailed explanation on how to use Direct Server Return (DSR) to build a highly scalable and available ingress for Kubernetes see the following blog post: https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/ What is DSR? When enabled, DSR allows the service endpoint to respond directly to the client request, bypassing the service proxy. When DSR is enabled kube-router will use LVS's tunneling mode to achieve this (more on how later). Quick Start You can enable DSR functionality on a per service basis. Requirements: * ClusterIP type service has an externalIP set on it or is a LoadBalancer type service * kube-router has been started with --service-external-ip-range configured at least once. This option can be specified multiple times for multiple ranges. The external IPs or LoadBalancer IPs must be included in these ranges. * kube-router must be run in service proxy mode with --run-service-proxy (this option is defaulted to true if left unspecified) * If you are advertising the service outside the cluster --advertise-external-ip must be set * If kube-router is deployed as a Kubernetes pod: * hostIPC: true must be set for the pod * hostPID: true must be set for the pod * The container runtime socket must be mounted into the kube-router pod via a hostPath volume mount. To enable DSR you need to annotate service with the kube-router.io/service.dsr=tunnel annotation: kubectl annotate service my-service \"kube-router.io/service.dsr=tunnel\" Things To Lookout For In the current implementation, DSR will only be available to the external IPs or LoadBalancer IPs The current implementation does not support port remapping. So you need to use same port and target port for the service. In order for DSR to work correctly, an ipip tunnel to the pod is used. This reduces the MTU for the packet by 20 bytes. Because of the way DSR works it is not possible for clients to use PMTU to discover this MTU reduction. In TCP based services, we mitigate this by using iptables to set the TCP MSS value to 20 bytes less than kube-router's primary interface MTU size. However, it is not possible to do this for UDP streams. Therefore, UDP streams that continuously use large packets may see a performance impact due to packet fragmentation. Additionally, if clients set the DF (Do Not Fragment) bit, services may see packet loss on UDP services. Kubernetes Pod Examples As mentioned previously, if kube-router is run as a Kubernetes deployment, there are a couple of things needed on the deployment. Below is an example of what is necessary to get going (this is NOT a full deployment, it is just meant to highlight the elements needed for DSR): apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: selector: matchLabels: k8s-app: kube-router tier: node template: metadata: labels: k8s-app: kube-router tier: node spec: hostNetwork: true hostIPC: true hostPID: true volumes: - name: run hostPath: path: /var/run/docker.sock ... containers: - name: kube-router image: docker.io/cloudnativelabs/kube-router:latest ... volumeMounts: - name: run mountPath: /var/run/docker.sock readOnly: true ... For an example manifest please look at the kube-router all features manifest with DSR requirements for Docker enabled. DSR with containerd or cri-o As of kube-router-1.2.X and later, kube-router's DSR mode now works with non-docker container runtimes. Officially only containerd has been tested, but this solution should work with cri-o as well. Most of what was said above also applies for non-docker container runtimes, however, there are some adjustments that you'll need to make: * You'll need to let kube-router know what container runtime socket to use via the --runtime-endpoint CLI parameter * If running kube-router as a Kubernetes deployment you'll need to make sure that you expose the correct socket via hostPath volume mount Here is an example kube-router daemonset manifest with just the changes needed to enable DSR with containerd (this is not a full manifest, it is just meant to highlight differences): apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-router spec: template: spec: ... volumes: - name: containerd-sock hostPath: path: /run/containerd/containerd.sock ... containers: - name: kube-router args: - --runtime-endpoint=unix:///run/containerd/containerd.sock ... volumeMounts: - name: containerd-sock mountPath: /run/containerd/containerd.sock readOnly: true ... More Details About DSR In order to facilitate troubleshooting it is worth while to explain how kube-router accomplishes DSR functionality. kube-router adds iptables rules to the mangle table which marks incoming packets destined for DSR based services with a unique FW mark. This mark is then used in later stages to identify the packet and route it correctly. Additionally, for TCP streams, there are rules that enable TCP MSS since the packets will change MTU when traversing an ipip tunnel later on. kube-router adds the marks to an ip rule (see: ip-rule(8) ). This ip rule then forces the incoming DSR service packets to use a specific routing table. kube-router adds a new ip route table (at the time of this writing the table number is 78 ) which forces the packet to route to the host even though there are no interfaces on the host that carry the DSR IP address kube-router adds an IPVS server configured for the custom FW mark. When packets arrive on the localhost interface because of the above ip rule and ip route , IPVS will intercept them based on their unique FW mark. When pods selected by the DSR service become ready, kube-router adds endpoints configured for tunnel mode to the above IPVS server. Each endpoint is configured in tunnel mode (as opposed to masquerade mode), which then encapsulates the incoming packet in an ipip packet. It is at this point that the pod's destination IP is placed on the ipip packet header so that a packet can be routed to the pod via the kube-bridge on either this host or the destination host. kube-router then finds the targeted pod and enters it's local network namespace. Once inside the pod's linux network namespace, it sets up two new interfaces called kube-dummy-if and ipip . kube-dummy-if is configured with the externalIP address of the service. When the ipip packet arrives inside the pod, the original source packet with the externalIP is then extracted from the ipip packet via the ipip interface and is accepted to the listening application via the kube-dummy-if interface. When the application sends its response back to the client, it responds to the client's public IP address (since that is what it saw on the request's IP header) and the packet is returned directly to the client (as opposed to traversing the Kubernetes internal network and potentially making multiple intermediate hops)","title":"DSR"},{"location":"dsr/#direct-server-return","text":"","title":"Direct Server Return"},{"location":"dsr/#more-information","text":"For a more detailed explanation on how to use Direct Server Return (DSR) to build a highly scalable and available ingress for Kubernetes see the following blog post: https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/","title":"More Information"},{"location":"dsr/#what-is-dsr","text":"When enabled, DSR allows the service endpoint to respond directly to the client request, bypassing the service proxy. When DSR is enabled kube-router will use LVS's tunneling mode to achieve this (more on how later).","title":"What is DSR?"},{"location":"dsr/#quick-start","text":"You can enable DSR functionality on a per service basis. Requirements: * ClusterIP type service has an externalIP set on it or is a LoadBalancer type service * kube-router has been started with --service-external-ip-range configured at least once. This option can be specified multiple times for multiple ranges. The external IPs or LoadBalancer IPs must be included in these ranges. * kube-router must be run in service proxy mode with --run-service-proxy (this option is defaulted to true if left unspecified) * If you are advertising the service outside the cluster --advertise-external-ip must be set * If kube-router is deployed as a Kubernetes pod: * hostIPC: true must be set for the pod * hostPID: true must be set for the pod * The container runtime socket must be mounted into the kube-router pod via a hostPath volume mount. To enable DSR you need to annotate service with the kube-router.io/service.dsr=tunnel annotation: kubectl annotate service my-service \"kube-router.io/service.dsr=tunnel\"","title":"Quick Start"},{"location":"dsr/#things-to-lookout-for","text":"In the current implementation, DSR will only be available to the external IPs or LoadBalancer IPs The current implementation does not support port remapping. So you need to use same port and target port for the service. In order for DSR to work correctly, an ipip tunnel to the pod is used. This reduces the MTU for the packet by 20 bytes. Because of the way DSR works it is not possible for clients to use PMTU to discover this MTU reduction. In TCP based services, we mitigate this by using iptables to set the TCP MSS value to 20 bytes less than kube-router's primary interface MTU size. However, it is not possible to do this for UDP streams. Therefore, UDP streams that continuously use large packets may see a performance impact due to packet fragmentation. Additionally, if clients set the DF (Do Not Fragment) bit, services may see packet loss on UDP services.","title":"Things To Lookout For"},{"location":"dsr/#kubernetes-pod-examples","text":"As mentioned previously, if kube-router is run as a Kubernetes deployment, there are a couple of things needed on the deployment. Below is an example of what is necessary to get going (this is NOT a full deployment, it is just meant to highlight the elements needed for DSR): apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: selector: matchLabels: k8s-app: kube-router tier: node template: metadata: labels: k8s-app: kube-router tier: node spec: hostNetwork: true hostIPC: true hostPID: true volumes: - name: run hostPath: path: /var/run/docker.sock ... containers: - name: kube-router image: docker.io/cloudnativelabs/kube-router:latest ... volumeMounts: - name: run mountPath: /var/run/docker.sock readOnly: true ... For an example manifest please look at the kube-router all features manifest with DSR requirements for Docker enabled.","title":"Kubernetes Pod Examples"},{"location":"dsr/#dsr-with-containerd-or-cri-o","text":"As of kube-router-1.2.X and later, kube-router's DSR mode now works with non-docker container runtimes. Officially only containerd has been tested, but this solution should work with cri-o as well. Most of what was said above also applies for non-docker container runtimes, however, there are some adjustments that you'll need to make: * You'll need to let kube-router know what container runtime socket to use via the --runtime-endpoint CLI parameter * If running kube-router as a Kubernetes deployment you'll need to make sure that you expose the correct socket via hostPath volume mount Here is an example kube-router daemonset manifest with just the changes needed to enable DSR with containerd (this is not a full manifest, it is just meant to highlight differences): apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-router spec: template: spec: ... volumes: - name: containerd-sock hostPath: path: /run/containerd/containerd.sock ... containers: - name: kube-router args: - --runtime-endpoint=unix:///run/containerd/containerd.sock ... volumeMounts: - name: containerd-sock mountPath: /run/containerd/containerd.sock readOnly: true ...","title":"DSR with containerd or cri-o"},{"location":"dsr/#more-details-about-dsr","text":"In order to facilitate troubleshooting it is worth while to explain how kube-router accomplishes DSR functionality. kube-router adds iptables rules to the mangle table which marks incoming packets destined for DSR based services with a unique FW mark. This mark is then used in later stages to identify the packet and route it correctly. Additionally, for TCP streams, there are rules that enable TCP MSS since the packets will change MTU when traversing an ipip tunnel later on. kube-router adds the marks to an ip rule (see: ip-rule(8) ). This ip rule then forces the incoming DSR service packets to use a specific routing table. kube-router adds a new ip route table (at the time of this writing the table number is 78 ) which forces the packet to route to the host even though there are no interfaces on the host that carry the DSR IP address kube-router adds an IPVS server configured for the custom FW mark. When packets arrive on the localhost interface because of the above ip rule and ip route , IPVS will intercept them based on their unique FW mark. When pods selected by the DSR service become ready, kube-router adds endpoints configured for tunnel mode to the above IPVS server. Each endpoint is configured in tunnel mode (as opposed to masquerade mode), which then encapsulates the incoming packet in an ipip packet. It is at this point that the pod's destination IP is placed on the ipip packet header so that a packet can be routed to the pod via the kube-bridge on either this host or the destination host. kube-router then finds the targeted pod and enters it's local network namespace. Once inside the pod's linux network namespace, it sets up two new interfaces called kube-dummy-if and ipip . kube-dummy-if is configured with the externalIP address of the service. When the ipip packet arrives inside the pod, the original source packet with the externalIP is then extracted from the ipip packet via the ipip interface and is accepted to the listening application via the kube-dummy-if interface. When the application sends its response back to the client, it responds to the client's public IP address (since that is what it saw on the request's IP header) and the packet is returned directly to the client (as opposed to traversing the Kubernetes internal network and potentially making multiple intermediate hops)","title":"More Details About DSR"},{"location":"generic/","text":"Kube-router on generic clusters This guide is for running kube-router as the CNI network provider for on premise and/or bare metal clusters outside of a cloud provider's environment. It assumes the initial cluster is bootstrapped and a networking provider needs configuration. All pod networking CIDRs are allocated by kube-controller-manager. Kube-router provides service/pod networking, a network policy firewall, and a high performance IPVS/LVS based service proxy. The network policy firewall and service proxy are both optional but recommended. Configuring the Kubelet If you choose to run kube-router as daemonset, then both kube-apiserver and kubelet must be run with --allow-privileged=true option Ensure each Kubelet is configured with the following options: --network-plugin=cni --cni-conf-dir=/etc/cni/net.d If running Kubelet containerised, make sure /etc/cni/net.d is mapped to the host's /etc/cni/net.d If a previous CNI provider (e.g. weave-net, calico, or flannel) was used, remove old configurations from /etc/cni/net.d on each kubelet. Note: Switching CNI providers on a running cluster requires re-creating all pods to pick up new pod IPs Configuring kube-controller-manager If you choose to use kube-router for pod-to-pod network connectivity then kube-controller-manager need to be configured to allocate pod CIDRs by passing --allocate-node-cidrs=true flag and providing a cluster-cidr (i.e. by passing --cluster-cidr=10.32.0.0/12 for e.g.) For example: --allocate-node-cidrs=true --cluster-cidr=10.32.0.0/12 --service-cluster-ip-range=10.50.0.0/22 Running kube-router with everything This runs kube-router in Kubernetes v1.8+ with pod/service networking, the network policy firewall, and service proxy to replace kube-proxy. The example command uses 10.32.0.0/12 as the pod CIDR address range and https://cluster01.int.domain.com:6443 as the apiserver address. Please change these to suit your cluster. CLUSTERCIDR=10.32.0.0/12 \\ APISERVER=https://cluster01.int.domain.com:6443 \\ sh -c 'curl https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml -o - | \\ sed -e \"s;%APISERVER%;$APISERVER;g\" -e \"s;%CLUSTERCIDR%;$CLUSTERCIDR;g\"' | \\ kubectl apply -f - Removing a previous kube-proxy If kube-proxy was never deployed to the cluster, this can likely be skipped. Remove any previously running kube-proxy and all iptables rules it created. Start by deleting the kube-proxy daemonset: kubectl -n kube-system delete ds kube-proxy Any iptables rules kube-proxy left around will also need to be cleaned up. This command might differ based on how kube-proxy was setup or configured: To cleanup kube-proxy we can do this with docker or containerd: docker: docker run --privileged -v /lib/modules:/lib/modules --net=host k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy --cleanup containerd: ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.23.4 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy-cleanup kube-proxy --cleanup Running kube-router without the service proxy This runs kube-router in Kubernetes v1.8+ with pod/service networking and the network policy firewall. The Services proxy is disabled. kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter.yaml In this mode kube-router relies on for example kube-proxy to provide service networking. When service proxy is disabled kube-router will use in-cluster configuration to access APIserver through cluster-ip. Service networking must therefore be setup before deploying kube-router. Debugging kube-router supports setting log level via the command line -v or --v, To get maximal debug output from kube-router please start with --v=3","title":"Kube-router on generic clusters"},{"location":"generic/#kube-router-on-generic-clusters","text":"This guide is for running kube-router as the CNI network provider for on premise and/or bare metal clusters outside of a cloud provider's environment. It assumes the initial cluster is bootstrapped and a networking provider needs configuration. All pod networking CIDRs are allocated by kube-controller-manager. Kube-router provides service/pod networking, a network policy firewall, and a high performance IPVS/LVS based service proxy. The network policy firewall and service proxy are both optional but recommended.","title":"Kube-router on generic clusters"},{"location":"generic/#configuring-the-kubelet","text":"If you choose to run kube-router as daemonset, then both kube-apiserver and kubelet must be run with --allow-privileged=true option Ensure each Kubelet is configured with the following options: --network-plugin=cni --cni-conf-dir=/etc/cni/net.d If running Kubelet containerised, make sure /etc/cni/net.d is mapped to the host's /etc/cni/net.d If a previous CNI provider (e.g. weave-net, calico, or flannel) was used, remove old configurations from /etc/cni/net.d on each kubelet. Note: Switching CNI providers on a running cluster requires re-creating all pods to pick up new pod IPs","title":"Configuring the Kubelet"},{"location":"generic/#configuring-kube-controller-manager","text":"If you choose to use kube-router for pod-to-pod network connectivity then kube-controller-manager need to be configured to allocate pod CIDRs by passing --allocate-node-cidrs=true flag and providing a cluster-cidr (i.e. by passing --cluster-cidr=10.32.0.0/12 for e.g.) For example: --allocate-node-cidrs=true --cluster-cidr=10.32.0.0/12 --service-cluster-ip-range=10.50.0.0/22","title":"Configuring kube-controller-manager"},{"location":"generic/#running-kube-router-with-everything","text":"This runs kube-router in Kubernetes v1.8+ with pod/service networking, the network policy firewall, and service proxy to replace kube-proxy. The example command uses 10.32.0.0/12 as the pod CIDR address range and https://cluster01.int.domain.com:6443 as the apiserver address. Please change these to suit your cluster. CLUSTERCIDR=10.32.0.0/12 \\ APISERVER=https://cluster01.int.domain.com:6443 \\ sh -c 'curl https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml -o - | \\ sed -e \"s;%APISERVER%;$APISERVER;g\" -e \"s;%CLUSTERCIDR%;$CLUSTERCIDR;g\"' | \\ kubectl apply -f -","title":"Running kube-router with everything"},{"location":"generic/#removing-a-previous-kube-proxy","text":"If kube-proxy was never deployed to the cluster, this can likely be skipped. Remove any previously running kube-proxy and all iptables rules it created. Start by deleting the kube-proxy daemonset: kubectl -n kube-system delete ds kube-proxy Any iptables rules kube-proxy left around will also need to be cleaned up. This command might differ based on how kube-proxy was setup or configured: To cleanup kube-proxy we can do this with docker or containerd: docker: docker run --privileged -v /lib/modules:/lib/modules --net=host k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy --cleanup containerd: ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.23.4 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy-cleanup kube-proxy --cleanup","title":"Removing a previous kube-proxy"},{"location":"generic/#running-kube-router-without-the-service-proxy","text":"This runs kube-router in Kubernetes v1.8+ with pod/service networking and the network policy firewall. The Services proxy is disabled. kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter.yaml In this mode kube-router relies on for example kube-proxy to provide service networking. When service proxy is disabled kube-router will use in-cluster configuration to access APIserver through cluster-ip. Service networking must therefore be setup before deploying kube-router.","title":"Running kube-router without the service proxy"},{"location":"generic/#debugging","text":"kube-router supports setting log level via the command line -v or --v, To get maximal debug output from kube-router please start with --v=3","title":"Debugging"},{"location":"health/","text":"Health checking kube-router kube-router currently has basic health checking in form of heartbeats sent from each controller to the healthcontroller each time the main loop completes successfully. The health port is by default 20244 but can be changed with the startup option. The health path is /healthz --health-port=<port number> If port is set to 0 (zero) no HTTP endpoint will be made availible but the health controller will still run and print out any missed heartbeats to STDERR of kube-router If a controller does not send a heartbeat within controllersynctime + 5 seconds the component will be flagged as unhealthy. If any of the running components is failing the whole kube-router state will be marked as failed in the /healthz endpoint E.g kube-router is started with --run-router=true --run-firewall=true --run-service-proxy=true If the route controller, policy controller or service controller exits it's main loop and does not publish a heartbeat the /healthz endpoint will return a error 500 signaling that kube-router is not healthy.","title":"Health"},{"location":"health/#health-checking-kube-router","text":"kube-router currently has basic health checking in form of heartbeats sent from each controller to the healthcontroller each time the main loop completes successfully. The health port is by default 20244 but can be changed with the startup option. The health path is /healthz --health-port=<port number> If port is set to 0 (zero) no HTTP endpoint will be made availible but the health controller will still run and print out any missed heartbeats to STDERR of kube-router If a controller does not send a heartbeat within controllersynctime + 5 seconds the component will be flagged as unhealthy. If any of the running components is failing the whole kube-router state will be marked as failed in the /healthz endpoint E.g kube-router is started with --run-router=true --run-firewall=true --run-service-proxy=true If the route controller, policy controller or service controller exits it's main loop and does not publish a heartbeat the /healthz endpoint will return a error 500 signaling that kube-router is not healthy.","title":"Health checking kube-router"},{"location":"how-it-works/","text":"Theory of Operation Kube-router can be run as an agent or a Pod (via DaemonSet) on each node and leverages standard Linux technologies iptables, ipvs/lvs, ipset, iproute2 Service Proxy And Load Balancing Blog: Kubernetes network services proxy with IPVS/LVS Kube-router uses IPVS/LVS technology built in Linux to provide L4 load balancing. Each ClusterIP , NodePort , and LoadBalancer Kubernetes Service type is configured as an IPVS virtual service. Each Service Endpoint is configured as real server to the virtual service. The standard ipvsadm tool can be used to verify the configuration and monitor the active connections. Below is example set of Services on Kubernetes: and the Endpoints for the Services: and how they got mapped to the IPVS by kube-router: Kube-router watches the Kubernetes API server to get updates on the Services/Endpoints and automatically syncs the IPVS configuration to reflect the desired state of Services. Kube-router uses IPVS masquerading mode and uses round robin scheduling currently. Source pod IP is preserved so that appropriate network policies can be applied. Pod Ingress Firewall Blog: Enforcing Kubernetes network policies with iptables Kube-router provides an implementation of Kubernetes Network Policies through the use of iptables, ipset and conntrack. All the Pods in a Namespace with 'DefaultDeny' ingress isolation policy has ingress blocked. Only traffic that matches whitelist rules specified in the network policies are permitted to reach those Pods. The following set of iptables rules and chains in the 'filter' table are used to achieve the Network Policies semantics. Each Pod running on the Node which needs ingress blocked by default is matched in FORWARD and OUTPUT chains of the fliter table and are sent to a pod specific firewall chain. Below rules are added to match various cases Traffic getting switched between the Pods on the same Node through the local bridge Traffic getting routed between the Pods on different Nodes Traffic originating from a Pod and going through the Service proxy and getting routed to a Pod on the same Node Each Pod specific firewall chain has default rule to block the traffic. Rules are added to jump traffic to the Network Policy specific policy chains. Rules cover only policies that apply to the destination pod ip. A rule is added to accept the the established traffic to permit the return traffic. Each policy chain has rules expressed through source and destination ipsets. Set of pods matching ingress rule in network policy spec forms a source Pod ip ipset. set of Pods matching pod selector (for destination Pods) in the Network Policy forms destination Pod ip ipset. Finally ipsets are created that are used in forming the rules in the Network Policy specific chain Kube-router at runtime watches Kubernetes API server for changes in the namespace, network policy and pods and dynamically updates iptables and ipset configuration to reflect desired state of ingress firewall for the the pods. Pod Networking Blog: Kubernetes pod networking and beyond with BGP Kube-router is expected to run on each Node. The subnet of the Node is obtained from the CNI configuration file on the Node or through the Node.PodCidr. Each kube-router instance on the Node acts as a BGP router and advertises the Pod CIDR assigned to the Node. Each Node peers with rest of the Nodes in the cluster forming full mesh. Learned routes about the Pod CIDR from the other Nodes (BGP peers) are injected into local Node routing table. On the data path, inter Node Pod-to-Pod communication is done by the routing stack on the Node.","title":"How it works"},{"location":"how-it-works/#theory-of-operation","text":"Kube-router can be run as an agent or a Pod (via DaemonSet) on each node and leverages standard Linux technologies iptables, ipvs/lvs, ipset, iproute2","title":"Theory of Operation"},{"location":"how-it-works/#service-proxy-and-load-balancing","text":"Blog: Kubernetes network services proxy with IPVS/LVS Kube-router uses IPVS/LVS technology built in Linux to provide L4 load balancing. Each ClusterIP , NodePort , and LoadBalancer Kubernetes Service type is configured as an IPVS virtual service. Each Service Endpoint is configured as real server to the virtual service. The standard ipvsadm tool can be used to verify the configuration and monitor the active connections. Below is example set of Services on Kubernetes: and the Endpoints for the Services: and how they got mapped to the IPVS by kube-router: Kube-router watches the Kubernetes API server to get updates on the Services/Endpoints and automatically syncs the IPVS configuration to reflect the desired state of Services. Kube-router uses IPVS masquerading mode and uses round robin scheduling currently. Source pod IP is preserved so that appropriate network policies can be applied.","title":"Service Proxy And Load Balancing"},{"location":"how-it-works/#pod-ingress-firewall","text":"Blog: Enforcing Kubernetes network policies with iptables Kube-router provides an implementation of Kubernetes Network Policies through the use of iptables, ipset and conntrack. All the Pods in a Namespace with 'DefaultDeny' ingress isolation policy has ingress blocked. Only traffic that matches whitelist rules specified in the network policies are permitted to reach those Pods. The following set of iptables rules and chains in the 'filter' table are used to achieve the Network Policies semantics. Each Pod running on the Node which needs ingress blocked by default is matched in FORWARD and OUTPUT chains of the fliter table and are sent to a pod specific firewall chain. Below rules are added to match various cases Traffic getting switched between the Pods on the same Node through the local bridge Traffic getting routed between the Pods on different Nodes Traffic originating from a Pod and going through the Service proxy and getting routed to a Pod on the same Node Each Pod specific firewall chain has default rule to block the traffic. Rules are added to jump traffic to the Network Policy specific policy chains. Rules cover only policies that apply to the destination pod ip. A rule is added to accept the the established traffic to permit the return traffic. Each policy chain has rules expressed through source and destination ipsets. Set of pods matching ingress rule in network policy spec forms a source Pod ip ipset. set of Pods matching pod selector (for destination Pods) in the Network Policy forms destination Pod ip ipset. Finally ipsets are created that are used in forming the rules in the Network Policy specific chain Kube-router at runtime watches Kubernetes API server for changes in the namespace, network policy and pods and dynamically updates iptables and ipset configuration to reflect desired state of ingress firewall for the the pods.","title":"Pod Ingress Firewall"},{"location":"how-it-works/#pod-networking","text":"Blog: Kubernetes pod networking and beyond with BGP Kube-router is expected to run on each Node. The subnet of the Node is obtained from the CNI configuration file on the Node or through the Node.PodCidr. Each kube-router instance on the Node acts as a BGP router and advertises the Pod CIDR assigned to the Node. Each Node peers with rest of the Nodes in the cluster forming full mesh. Learned routes about the Pod CIDR from the other Nodes (BGP peers) are injected into local Node routing table. On the data path, inter Node Pod-to-Pod communication is done by the routing stack on the Node.","title":"Pod Networking"},{"location":"introduction/","text":"Introduction Welcome to the introduction guide to Kube-router! This guide is the best place to start with Kube-router. We cover what Kube-router is, what problems it can solve, how it compares to existing software, and how you can get started using it. If you are familiar with the basics of Kube-router, head over to the next sections that provide a more detailed reference of available features. What is Kube-router If you are not familiar with Kubernetes networking model it is recommended to familiarize with Kubernetes networking model . So essentially Kubernetes expects: all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as Kubernetes only prescribes the requirements for the networking model but does not provide any default implementation. For a functional Kubernetes cluster one has to deploy what is called as CNI or pod networking solution that provides above functionality. Any non-trivial containerized application will end up running multiple pods running different services. Service abstraction in Kubernetes is an essential building block that helps in service discovery and load balancing. A layer-4 service proxy must be deployed to Kubernetes cluster that provides the load-balancing for the services exposed by the pods. Once you have pod-to-pod networking established and have a service proxy that provides load-balancing, you need a way to secure your pods. Kubernetes Network Policies provides a specfication on how to secure pods. You need to deploy a solution that implements network policy specification and provides security to your pods. Kube-router is a turnkey solution for Kubernetes networking that provides all the above essential functionality in one single elegant package. Why Kube-router Network is hard. You have multiple Kubernetes networking solutions that provide pod networking or network policy etc. But when you deploy indiviudal solution for each functionality you end up with lot of moving parts making it difficult to operate and troubleshoot. Kube-router is a lean yet powerful all-in-one alternative to several network components used in typical Kubernetes clusters. All this from a single DaemonSet/Binary. It doesn't get any easier. Kube-router also uses best of the solution for maximum performance. Kube-router uses IPVS/LVS for service proxy and provides direct routing between the nodes. Kube-router also provides very unique and advanced functionalities like DSR (Direct Server Return), ECMP based network load balancing etc","title":"Introduction"},{"location":"introduction/#introduction","text":"Welcome to the introduction guide to Kube-router! This guide is the best place to start with Kube-router. We cover what Kube-router is, what problems it can solve, how it compares to existing software, and how you can get started using it. If you are familiar with the basics of Kube-router, head over to the next sections that provide a more detailed reference of available features.","title":"Introduction"},{"location":"introduction/#what-is-kube-router","text":"If you are not familiar with Kubernetes networking model it is recommended to familiarize with Kubernetes networking model . So essentially Kubernetes expects: all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as Kubernetes only prescribes the requirements for the networking model but does not provide any default implementation. For a functional Kubernetes cluster one has to deploy what is called as CNI or pod networking solution that provides above functionality. Any non-trivial containerized application will end up running multiple pods running different services. Service abstraction in Kubernetes is an essential building block that helps in service discovery and load balancing. A layer-4 service proxy must be deployed to Kubernetes cluster that provides the load-balancing for the services exposed by the pods. Once you have pod-to-pod networking established and have a service proxy that provides load-balancing, you need a way to secure your pods. Kubernetes Network Policies provides a specfication on how to secure pods. You need to deploy a solution that implements network policy specification and provides security to your pods. Kube-router is a turnkey solution for Kubernetes networking that provides all the above essential functionality in one single elegant package.","title":"What is Kube-router"},{"location":"introduction/#why-kube-router","text":"Network is hard. You have multiple Kubernetes networking solutions that provide pod networking or network policy etc. But when you deploy indiviudal solution for each functionality you end up with lot of moving parts making it difficult to operate and troubleshoot. Kube-router is a lean yet powerful all-in-one alternative to several network components used in typical Kubernetes clusters. All this from a single DaemonSet/Binary. It doesn't get any easier. Kube-router also uses best of the solution for maximum performance. Kube-router uses IPVS/LVS for service proxy and provides direct routing between the nodes. Kube-router also provides very unique and advanced functionalities like DSR (Direct Server Return), ECMP based network load balancing etc","title":"Why Kube-router"},{"location":"ipv6/","text":"Ipv6 support in kube-router This document describes the current status, the plan ahead and general thoughts about ipv6 support in kube-router . Ipv6-only is supported (in alpha) in Kubernetes from version 1.9 and support for ipv4/ipv6 dual stack is on the way ( KEP ). It is desirable for kube-router to keep up with that development. The idea is to implement ipv6-only function-by-function; CNI --enable-cni Proxy --run-service-proxy Router --run-router Network policies --run-firewall It is important to always keep dual-stack in mind. The code must not be altered to handle ipv6 instead of ipv4 but must be able to handle both at the same time in the near future. To use ipv6 is usually not so hard in golang. The same code is used, only the addresses differ. This is also true for iptables and ipvsadm . This makes support for ipv6 a bit easier to implement. Testing Test and development has so far used https://github.com/Nordix/xcluster/tree/master/ovl/kube-router-ipv6 which is an easy way to get a ipv6-only Kubernetes cluster. To setup an ipv6-only Kubernetes cluster is usually no simple task, see for instance https://github.com/leblancd/kube-v6 No automatic tests exist yet for ipv6. Current status (Thu Oct 11 2018) Support for ipv6 in the the CNI function in kube-router is under development. The local BGP routers peers with ipv6; # gobgp neighbor Peer AS Up/Down State |#Received Accepted 1000::1:c0a8:101 64512 00:00:37 Establ | 0 0 1000::1:c0a8:102 64512 00:00:37 Establ | 0 0 1000::1:c0a8:103 64512 00:00:40 Establ | 0 0 The CNI configuration is also updated with ipv6 addresses; # jq . < /etc/cni/net.d/10-kuberouter.conf | cat { \"bridge\": \"kube-bridge\", \"ipam\": { \"subnet\": \"1000::2:b00:100/120\", \"type\": \"host-local\" }, \"isDefaultGateway\": true, \"isGateway\": true, \"name\": \"ekvm\", \"type\": \"bridge\" } This means that pod's gets assigned ipv6 addresses. The announcement of the pod CIDRs does not work yet. So pods on other nodes than the own cannot be reached. To get this working the routes must be inserted in the RIB for gobgp . Checking the ipv4 rib gives an error; # gobgp -a ipv4 global rib invalid nexthop address: <nil> While the ipv6 rib is empty; # gobgp -a ipv6 global rib Network not in table A guess is that kube-router tries to insert ipv6 addresses in the ipv4 rib. When the bgp announcement of ipv6 cidr for pods work the support for ipv6 in the kube-router CNI is done (I hope). Roadmap There is no time-plan. Help is welcome. After the CNI the next function in line may be the service proxy. ipvs has full support for ipv6. The dual-stack KEP states that to get dual stack support for a service two services must be specified, one for ipv4 and another for ipv6. The implementation should get the protocol from a global setting for ipv6-only and later from some attribute in the service object. Since the same gobgp is used for the CNI and the router functions is may be fairly simple to implement ipv6 support. Ipv6 support for the firewall function is not investigated. Ipv6 support for ipset is implemented already for the CNI.","title":"Ipv6 support in kube-router"},{"location":"ipv6/#ipv6-support-in-kube-router","text":"This document describes the current status, the plan ahead and general thoughts about ipv6 support in kube-router . Ipv6-only is supported (in alpha) in Kubernetes from version 1.9 and support for ipv4/ipv6 dual stack is on the way ( KEP ). It is desirable for kube-router to keep up with that development. The idea is to implement ipv6-only function-by-function; CNI --enable-cni Proxy --run-service-proxy Router --run-router Network policies --run-firewall It is important to always keep dual-stack in mind. The code must not be altered to handle ipv6 instead of ipv4 but must be able to handle both at the same time in the near future. To use ipv6 is usually not so hard in golang. The same code is used, only the addresses differ. This is also true for iptables and ipvsadm . This makes support for ipv6 a bit easier to implement.","title":"Ipv6 support in kube-router"},{"location":"ipv6/#testing","text":"Test and development has so far used https://github.com/Nordix/xcluster/tree/master/ovl/kube-router-ipv6 which is an easy way to get a ipv6-only Kubernetes cluster. To setup an ipv6-only Kubernetes cluster is usually no simple task, see for instance https://github.com/leblancd/kube-v6 No automatic tests exist yet for ipv6.","title":"Testing"},{"location":"ipv6/#current-status-thu-oct-11-2018","text":"Support for ipv6 in the the CNI function in kube-router is under development. The local BGP routers peers with ipv6; # gobgp neighbor Peer AS Up/Down State |#Received Accepted 1000::1:c0a8:101 64512 00:00:37 Establ | 0 0 1000::1:c0a8:102 64512 00:00:37 Establ | 0 0 1000::1:c0a8:103 64512 00:00:40 Establ | 0 0 The CNI configuration is also updated with ipv6 addresses; # jq . < /etc/cni/net.d/10-kuberouter.conf | cat { \"bridge\": \"kube-bridge\", \"ipam\": { \"subnet\": \"1000::2:b00:100/120\", \"type\": \"host-local\" }, \"isDefaultGateway\": true, \"isGateway\": true, \"name\": \"ekvm\", \"type\": \"bridge\" } This means that pod's gets assigned ipv6 addresses. The announcement of the pod CIDRs does not work yet. So pods on other nodes than the own cannot be reached. To get this working the routes must be inserted in the RIB for gobgp . Checking the ipv4 rib gives an error; # gobgp -a ipv4 global rib invalid nexthop address: <nil> While the ipv6 rib is empty; # gobgp -a ipv6 global rib Network not in table A guess is that kube-router tries to insert ipv6 addresses in the ipv4 rib. When the bgp announcement of ipv6 cidr for pods work the support for ipv6 in the kube-router CNI is done (I hope).","title":"Current status (Thu Oct 11 2018)"},{"location":"ipv6/#roadmap","text":"There is no time-plan. Help is welcome. After the CNI the next function in line may be the service proxy. ipvs has full support for ipv6. The dual-stack KEP states that to get dual stack support for a service two services must be specified, one for ipv4 and another for ipv6. The implementation should get the protocol from a global setting for ipv6-only and later from some attribute in the service object. Since the same gobgp is used for the CNI and the router functions is may be fairly simple to implement ipv6 support. Ipv6 support for the firewall function is not investigated. Ipv6 support for ipset is implemented already for the CNI.","title":"Roadmap"},{"location":"kops/","text":"Kops Integration Kops version 1.6.2 and above now officially includes kube-router integration. Please follow the instruction at https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer to provision a Kubernetes cluster with Kube-router. Uses the kops latest version binaries which has the kube-router support.","title":"kops"},{"location":"kops/#kops-integration","text":"Kops version 1.6.2 and above now officially includes kube-router integration. Please follow the instruction at https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer to provision a Kubernetes cluster with Kube-router. Uses the kops latest version binaries which has the kube-router support.","title":"Kops Integration"},{"location":"kubeadm/","text":"Deploying kube-router with kubeadm Please follow the steps to install Kubernetes cluster with Kubeadm, however must specify --pod-network-cidr when you run kubeadm init . Kube-router relies on kube-controller-manager to allocate pod CIDR for the nodes. Kube-router provides pod networking, network policy and high perfoming IPVS/LVS based service proxy. Depending on you choose to use kube-router for service proxy you have two options. kube-router providing pod networking and network policy For the step #3 Installing a pod network install a kube-router pod network and network policy add-on with the following command (Kubernetes version should be at least 1.8): KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml kube-router providing service proxy, firewall and pod networking. For the step #3 Installing a pod network install a kube-router pod network and network policy add-on with the following command (Kubernetes version should be at least 1.8): KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml Now since kube-router provides service proxy as well. Run below commands to remove kube-proxy and cleanup any iptables configuration it may have done. KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n kube-system delete ds kube-proxy To cleanup kube-proxy we can do this with docker or containerd: docker: docker run --privileged -v /lib/modules:/lib/modules --net=host k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy --cleanup containerd: ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.23.4 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy-cleanup kube-proxy --cleanup","title":"kubeadm"},{"location":"kubeadm/#deploying-kube-router-with-kubeadm","text":"Please follow the steps to install Kubernetes cluster with Kubeadm, however must specify --pod-network-cidr when you run kubeadm init . Kube-router relies on kube-controller-manager to allocate pod CIDR for the nodes. Kube-router provides pod networking, network policy and high perfoming IPVS/LVS based service proxy. Depending on you choose to use kube-router for service proxy you have two options.","title":"Deploying kube-router with kubeadm"},{"location":"kubeadm/#kube-router-providing-pod-networking-and-network-policy","text":"For the step #3 Installing a pod network install a kube-router pod network and network policy add-on with the following command (Kubernetes version should be at least 1.8): KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml","title":"kube-router providing pod networking and network policy"},{"location":"kubeadm/#kube-router-providing-service-proxy-firewall-and-pod-networking","text":"For the step #3 Installing a pod network install a kube-router pod network and network policy add-on with the following command (Kubernetes version should be at least 1.8): KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml Now since kube-router provides service proxy as well. Run below commands to remove kube-proxy and cleanup any iptables configuration it may have done. KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n kube-system delete ds kube-proxy To cleanup kube-proxy we can do this with docker or containerd: docker: docker run --privileged -v /lib/modules:/lib/modules --net=host k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy --cleanup containerd: ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.23.4 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ k8s.gcr.io/kube-proxy-amd64:v1.23.4 kube-proxy-cleanup kube-proxy --cleanup","title":"kube-router providing service proxy, firewall and pod networking."},{"location":"metrics/","text":"Metrics Scraping kube-router metrics with Prometheus The scope of this document is to describe how to setup the annotations needed for Prometheus to use Kubernetes SD to discover & scape kube-router pods . For help with installing Prometheus please see their docs Metrics options: --metrics-path string Path to serve Prometheus metrics on ( default: /metrics ) --metrics-port uint16 <0-65535> Prometheus metrics port to use ( default: 0, disabled ) To enable kube-router metrics, start kube-router with --metrics-port and provide a port over 0 Metrics is generally exported at the same rate as the sync period for each service. The default values unless other specified is iptables-sync-period - 1 min ipvs-sync-period - 1 min routes-sync-period - 1 min By enabling Kubernetes SD in Prometheus configuration & adding required annotations Prometheus can automaticly discover & scrape kube-router metrics Version notes kube-router v0.2.4 received a metrics overhaul where some metrics were changed into histograms, additional metrics was also added. Please make sure you are using the latest dashboard version with versions => v0.2.4 kube-router 0.1.0-rc2 and upwards supports the runtime configuration for controlling where to expose the metrics. If you are using a older version, metrics path & port is locked to /metrics & 8080 Supported annotations The following annotations can be set on pods/services to enable automatic SD & scraping prometheus.io/scrape : Only scrape services that have a value of true prometheus.io/path : If the metrics path is not /metrics override this. prometheus.io/port : If the metrics are exposed on a different port to the They are to be set under spec.template.metadata For example: spec: template: metadata: labels: k8s-app: kube-router annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" Avail metrics If metrics is enabled only the running services metrics are exposed The following metrics is exposed by kube-router prefixed by kube_router_ run-router = true controller_bgp_peers Number of BGP peers of the instance controller_bgp_advertisements_received Total number of BGP advertisements received since kube-router started controller_bgp_advertisements_sent Total number of BGP advertisements sent since kube-router started controller_bgp_internal_peers_sync_time Time it took for the BGP internal peer sync loop to complete controller_routes_sync_time Time it took for controller to sync routes run-firewall=true controller_iptables_sync_time Time it took for the iptables sync loop to complete controller_policy_chains_sync_time Time it took for controller to sync policy chains run-service-proxy = true controller_ipvs_services_sync_time Time it took for the ipvs sync loop to complete controller_ipvs_services The number of ipvs services in the instance controller_ipvs_metrics_export_time The time it took to run the metrics export for IPVS services service_total_connections Total connections made to the service since creation service_packets_in Total n/o packets received by service service_packets_out Total n/o packets sent by service service_bytes_in Total bytes received by the service service_bytes_out Total bytes sent by the service service_pps_in Incoming packets per second service_pps_out Outgoing packets per second service_cps Connections per second service_bps_in Incoming bytes per second service_bps_out Outgoing bytes per second To get a grouped list of CPS for each service a Prometheus query could look like this e.g: sum(kube_router_service_cps) by (svc_namespace, service_name) Grafana Dashboard This repo contains a example Grafana dashboard utilizing all the above exposed metrics from kube-router.","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#scraping-kube-router-metrics-with-prometheus","text":"The scope of this document is to describe how to setup the annotations needed for Prometheus to use Kubernetes SD to discover & scape kube-router pods . For help with installing Prometheus please see their docs Metrics options: --metrics-path string Path to serve Prometheus metrics on ( default: /metrics ) --metrics-port uint16 <0-65535> Prometheus metrics port to use ( default: 0, disabled ) To enable kube-router metrics, start kube-router with --metrics-port and provide a port over 0 Metrics is generally exported at the same rate as the sync period for each service. The default values unless other specified is iptables-sync-period - 1 min ipvs-sync-period - 1 min routes-sync-period - 1 min By enabling Kubernetes SD in Prometheus configuration & adding required annotations Prometheus can automaticly discover & scrape kube-router metrics","title":"Scraping kube-router metrics with Prometheus"},{"location":"metrics/#version-notes","text":"kube-router v0.2.4 received a metrics overhaul where some metrics were changed into histograms, additional metrics was also added. Please make sure you are using the latest dashboard version with versions => v0.2.4 kube-router 0.1.0-rc2 and upwards supports the runtime configuration for controlling where to expose the metrics. If you are using a older version, metrics path & port is locked to /metrics & 8080","title":"Version notes"},{"location":"metrics/#supported-annotations","text":"The following annotations can be set on pods/services to enable automatic SD & scraping prometheus.io/scrape : Only scrape services that have a value of true prometheus.io/path : If the metrics path is not /metrics override this. prometheus.io/port : If the metrics are exposed on a different port to the They are to be set under spec.template.metadata For example: spec: template: metadata: labels: k8s-app: kube-router annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\"","title":"Supported annotations"},{"location":"metrics/#avail-metrics","text":"If metrics is enabled only the running services metrics are exposed The following metrics is exposed by kube-router prefixed by kube_router_","title":"Avail metrics"},{"location":"metrics/#run-router-true","text":"controller_bgp_peers Number of BGP peers of the instance controller_bgp_advertisements_received Total number of BGP advertisements received since kube-router started controller_bgp_advertisements_sent Total number of BGP advertisements sent since kube-router started controller_bgp_internal_peers_sync_time Time it took for the BGP internal peer sync loop to complete controller_routes_sync_time Time it took for controller to sync routes","title":"run-router = true"},{"location":"metrics/#run-firewalltrue","text":"controller_iptables_sync_time Time it took for the iptables sync loop to complete controller_policy_chains_sync_time Time it took for controller to sync policy chains","title":"run-firewall=true"},{"location":"metrics/#run-service-proxy-true","text":"controller_ipvs_services_sync_time Time it took for the ipvs sync loop to complete controller_ipvs_services The number of ipvs services in the instance controller_ipvs_metrics_export_time The time it took to run the metrics export for IPVS services service_total_connections Total connections made to the service since creation service_packets_in Total n/o packets received by service service_packets_out Total n/o packets sent by service service_bytes_in Total bytes received by the service service_bytes_out Total bytes sent by the service service_pps_in Incoming packets per second service_pps_out Outgoing packets per second service_cps Connections per second service_bps_in Incoming bytes per second service_bps_out Outgoing bytes per second To get a grouped list of CPS for each service a Prometheus query could look like this e.g: sum(kube_router_service_cps) by (svc_namespace, service_name)","title":"run-service-proxy = true"},{"location":"metrics/#grafana-dashboard","text":"This repo contains a example Grafana dashboard utilizing all the above exposed metrics from kube-router.","title":"Grafana Dashboard"},{"location":"pod-toolbox/","text":"Pod Toolbox When kube-router is ran as a Pod within your Kubernetes cluster, it also ships with a number of tools automatically configured for your cluster. These can be used to troubleshoot issues and learn more about how cluster networking is performed. Logging In Here's a quick way to get going on a random node in your cluster: KR_POD=$(basename $(kubectl -n kube-system get pods -l k8s-app=kube-router --output name|head -n1)) kubectl -n kube-system exec -it ${KR_POD} bash Use kubectl -n kube-system get pods -l k8s-app=kube-router -o wide to see what nodes are running which pods. This will help if you want to investigate a particular node. Tools And Usage Once logged in you will see some help on using the tools in the container. For example: Welcome to kube-router on \"node1.zbrbdl\"! For debugging, the following tools are available: - ipvsadm | Gather info about Virtual Services and Real Servers via IPVS. | Examples: | ## Show all options | ipvsadm --help | ## List Services and Endpoints handled by IPVS | ipvsadm -ln | ## Show traffic rate information | ipvsadm -ln --rate | ## Show cumulative traffic | ipvsadm -ln --stats - gobgp | Get BGP related information from your nodes. | | Tab-completion is ready to use, just type \"gobgp <TAB>\" | to see the subcommands available. | | By default gobgp will query the Node this Pod is running | on, i.e. \"node1.zbrbdl\". To query a different node use | \"gobgp --host node02.mydomain\" as an example. | | For more examples see: https://github.com/osrg/gobgp/blob/master/docs/sources/cli-command-syntax.md Here's a quick look at what's happening on this Node --- BGP Server Configuration --- AS: 64512 Router-ID: 10.10.3.2 Listening Port: 179, Addresses: 0.0.0.0, :: --- BGP Neighbors --- Peer AS Up/Down State |#Received Accepted 64512 2d 01:05:07 Establ | 1 1 --- BGP Route Info --- Network Next Hop AS_PATH Age Attrs *> 10.2.0.0/24 10.10.3.3 4000 400000 300000 40001 2d 01:05:20 [{Origin: i} {LocalPref: 100}] *> 10.2.1.0/24 10.10.3.2 4000 400000 300000 40001 00:00:36 [{Origin: i}] --- IPVS Services --- IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.3.0.1:443 rr persistent 10800 mask 0.0.0.0 -> 10.10.3.2:443 Masq 1 0 0 TCP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0 TCP 10.3.0.15:2379 rr -> 10.10.3.3:2379 Masq 1 45 0 TCP 10.3.0.155:2379 rr -> 10.10.3.3:2379 Masq 1 0 0 UDP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0","title":"Pod tool box"},{"location":"pod-toolbox/#pod-toolbox","text":"When kube-router is ran as a Pod within your Kubernetes cluster, it also ships with a number of tools automatically configured for your cluster. These can be used to troubleshoot issues and learn more about how cluster networking is performed.","title":"Pod Toolbox"},{"location":"pod-toolbox/#logging-in","text":"Here's a quick way to get going on a random node in your cluster: KR_POD=$(basename $(kubectl -n kube-system get pods -l k8s-app=kube-router --output name|head -n1)) kubectl -n kube-system exec -it ${KR_POD} bash Use kubectl -n kube-system get pods -l k8s-app=kube-router -o wide to see what nodes are running which pods. This will help if you want to investigate a particular node.","title":"Logging In"},{"location":"pod-toolbox/#tools-and-usage","text":"Once logged in you will see some help on using the tools in the container. For example: Welcome to kube-router on \"node1.zbrbdl\"! For debugging, the following tools are available: - ipvsadm | Gather info about Virtual Services and Real Servers via IPVS. | Examples: | ## Show all options | ipvsadm --help | ## List Services and Endpoints handled by IPVS | ipvsadm -ln | ## Show traffic rate information | ipvsadm -ln --rate | ## Show cumulative traffic | ipvsadm -ln --stats - gobgp | Get BGP related information from your nodes. | | Tab-completion is ready to use, just type \"gobgp <TAB>\" | to see the subcommands available. | | By default gobgp will query the Node this Pod is running | on, i.e. \"node1.zbrbdl\". To query a different node use | \"gobgp --host node02.mydomain\" as an example. | | For more examples see: https://github.com/osrg/gobgp/blob/master/docs/sources/cli-command-syntax.md Here's a quick look at what's happening on this Node --- BGP Server Configuration --- AS: 64512 Router-ID: 10.10.3.2 Listening Port: 179, Addresses: 0.0.0.0, :: --- BGP Neighbors --- Peer AS Up/Down State |#Received Accepted 64512 2d 01:05:07 Establ | 1 1 --- BGP Route Info --- Network Next Hop AS_PATH Age Attrs *> 10.2.0.0/24 10.10.3.3 4000 400000 300000 40001 2d 01:05:20 [{Origin: i} {LocalPref: 100}] *> 10.2.1.0/24 10.10.3.2 4000 400000 300000 40001 00:00:36 [{Origin: i}] --- IPVS Services --- IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.3.0.1:443 rr persistent 10800 mask 0.0.0.0 -> 10.10.3.2:443 Masq 1 0 0 TCP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0 TCP 10.3.0.15:2379 rr -> 10.10.3.3:2379 Masq 1 45 0 TCP 10.3.0.155:2379 rr -> 10.10.3.3:2379 Masq 1 0 0 UDP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0","title":"Tools And Usage"},{"location":"see-it-in-action/","text":"See Kube-router in action Network Services Controller Network services controller is responsible for reading the services and endpoints information from Kubernetes API server and configure IPVS on each cluster node accordingly. Please our read blog for design details and pros and cons compared to iptables based Kube-proxy https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/ Demo of Kube-router's IPVS based Kubernetes network service proxy Features: - round robin load balancing - client IP based session persistence - source IP is preserved if service controller is used in conjuction with network routes controller (kube-router with --run-router flag) - option to explicitly masquerade (SNAT) with --masquerade-all flag Network Policy Controller Network policy controller is responsible for reading the namespace, network policy and pods information from Kubernetes API server and configure iptables accordingly to provide ingress filter to the pods. Kube-router supports the networking.k8s.io/NetworkPolicy API or network policy V1/GA semantics and also network policy beta semantics. Please read blog for design details of Network Policy controller https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/ Demo of Kube-router's iptables based implementaton of network policies Network Routes Controller Network routes controller is responsible for reading pod CIDR allocated by controller manager to the node, and advertises the routes to the rest of the nodes in the cluster (BGP peers). Use of BGP is transperent to user for basic pod-to-pod networking. However BGP can be leveraged to other use cases like advertising the cluster ip, routable pod ip etc. Only in such use-cases understanding of BGP and configuration is required. Please see below demo how kube-router advertises cluster IP and pod cidrs to external BGP router","title":"See it in Action"},{"location":"see-it-in-action/#see-kube-router-in-action","text":"","title":"See Kube-router in action"},{"location":"see-it-in-action/#network-services-controller","text":"Network services controller is responsible for reading the services and endpoints information from Kubernetes API server and configure IPVS on each cluster node accordingly. Please our read blog for design details and pros and cons compared to iptables based Kube-proxy https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/ Demo of Kube-router's IPVS based Kubernetes network service proxy Features: - round robin load balancing - client IP based session persistence - source IP is preserved if service controller is used in conjuction with network routes controller (kube-router with --run-router flag) - option to explicitly masquerade (SNAT) with --masquerade-all flag","title":"Network Services Controller"},{"location":"see-it-in-action/#network-policy-controller","text":"Network policy controller is responsible for reading the namespace, network policy and pods information from Kubernetes API server and configure iptables accordingly to provide ingress filter to the pods. Kube-router supports the networking.k8s.io/NetworkPolicy API or network policy V1/GA semantics and also network policy beta semantics. Please read blog for design details of Network Policy controller https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/ Demo of Kube-router's iptables based implementaton of network policies","title":"Network Policy Controller"},{"location":"see-it-in-action/#network-routes-controller","text":"Network routes controller is responsible for reading pod CIDR allocated by controller manager to the node, and advertises the routes to the rest of the nodes in the cluster (BGP peers). Use of BGP is transperent to user for basic pod-to-pod networking. However BGP can be leveraged to other use cases like advertising the cluster ip, routable pod ip etc. Only in such use-cases understanding of BGP and configuration is required. Please see below demo how kube-router advertises cluster IP and pod cidrs to external BGP router","title":"Network Routes Controller"},{"location":"troubleshoot/","text":"","title":"Troubleshoot"},{"location":"upgrading/","text":"Upgrading kube-router Breaking Changes We follow semantic versioning and try to the best of our abilities to maintain a stable interface between patch versions. For example, v0.1.1 -> v0.1.2 should be a perfectly safe upgrade path, without data service interruption. However, major ( vX.0.0 ) and minor ( v0.Y.0 ) version upgrades may contain breaking changes, which will be detailed here and in the release notes. First check if you are upgrading across one of the breaking change versions . If so, read the relevant section(s) first before proceeding with the general guidelines below. General Guidelines Image Pull Policy Here we will assume that you have the following in your kube-router DaemonSet: imagePullPolicy: Always If that's not the case, you will need to manually pull the desired image version on each of your nodes with a command like: docker pull cloudnativelabs/kube-router:VERSION Without Rolling Updates This is the default situation with our DaemonSet manifests. We will soon be switching these manifests to use Rolling Updates though. The following example(s) show an upgrade from v0.0.15 to v0.0.16 . First we will modify the kube-router DaemonSet resource's image field: kubectl -n kube-system set image ds/kube-router kube-router=cloudnativelabs/kube-router:v0.0.16 This does not actually trigger any version changes yet. It is recommended that you upgrade only one node and perform any tests you see fit to ensure nothing goes wrong. For example, we'll test upgrading kube-router on worker-01: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system delete pod \"${TEST_POD}\" You can watch to make sure the new kube-router pod comes up and stays running with: kubectl -n kube-system get pods -o wide -w Check the logs with: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system logs \"${TEST_POD}\" If it all looks good, go ahead and upgrade kube-router on all nodes: kubectl -n kube-system delete pods -l k8s-app=kube-router With Rolling Updates After updating a DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods will be created automatically, in a controlled fashion If your global BGP peers supports gracefull restarts and has it enabled, rolling updates can be used to upgrade your kube-router DaemonSet without network downtime To enable gracefull BGP restart kube-router must be started with --bgp-graceful-restart To enable rolling updates on your kube-router DaemonSet modify it and add a updateStrategy updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxUnavailable controls the maximum number of pods to simultaneously upgrade Starting from the top of the DaemonSet, it should look like this after you are done editing apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 ... Breaking Change Version History This section covers version specific upgrade instructions. v0.0.X alpha versions While kube-router is in its alpha stage changes can be expected to be rapid. Therefor we cannot guarantee that a new alpha release will not break previous expected behavior. v0.0.17 (aka v0.1.0-rc1) This version brings changes to hairpin and BGP peering CLI/annotation configuration flags/keys. CLI flag changes: - OLD: --peer-router -> NEW: --peer-router-ips - OLD: --peer-asn -> NEW: --peer-router-asns CLI flag additions: - NEW: --peer-router-passwords Annotation key changes: - OLD: kube-router.io/hairpin-mode= -> NEW: kube-router.io/service.hairpin= - OLD: net.kuberouter.nodeasn= -> NEW: kube-router.io/node.asn= - OLD: net.kuberouter.node.bgppeer.address= -> NEW: kube-router.io/peer.ips - OLD: net.kuberouter.node.bgppeer.asn -> NEW: kube-router.io/peer.asns Annotation key additions: - NEW: kube-router.io/peer.passwords v0.0.17 Upgrade Procedure For CLI flag changes, all that is required is to change the flag names you use above to their new names at the same time that you change the image version. kubectl -n kube-system edit ds kube-router For Annotations, the recommended approach is to copy all the values of your current annotations into new annotations with the updated keys. You can get a quick look at all your service and node annotations with these commands: kubectl describe services --all-namespaces |grep -E '^(Name:|Annotations:)' kubectl describe nodes |grep -E '^(Name:|Annotations:)' For example if you have a service annotation to enable Hairpin mode like: Name: hairpin-service Annotations: kube-router.io/hairpin-mode= You will then want to make a new annotation with the new key: kubectl annotate service hairpin-service \"kube-router.io/service.hairpin=\" Once all new annotations are created, proceed with the General Guidelines . After the upgrades tested and complete, you can delete the old annotations. kubectl annotate service hairpin-service \"kube-router.io/hairpin-mode-\"","title":"Upgrading"},{"location":"upgrading/#upgrading-kube-router","text":"","title":"Upgrading kube-router"},{"location":"upgrading/#breaking-changes","text":"We follow semantic versioning and try to the best of our abilities to maintain a stable interface between patch versions. For example, v0.1.1 -> v0.1.2 should be a perfectly safe upgrade path, without data service interruption. However, major ( vX.0.0 ) and minor ( v0.Y.0 ) version upgrades may contain breaking changes, which will be detailed here and in the release notes. First check if you are upgrading across one of the breaking change versions . If so, read the relevant section(s) first before proceeding with the general guidelines below.","title":"Breaking Changes"},{"location":"upgrading/#general-guidelines","text":"","title":"General Guidelines"},{"location":"upgrading/#image-pull-policy","text":"Here we will assume that you have the following in your kube-router DaemonSet: imagePullPolicy: Always If that's not the case, you will need to manually pull the desired image version on each of your nodes with a command like: docker pull cloudnativelabs/kube-router:VERSION","title":"Image Pull Policy"},{"location":"upgrading/#without-rolling-updates","text":"This is the default situation with our DaemonSet manifests. We will soon be switching these manifests to use Rolling Updates though. The following example(s) show an upgrade from v0.0.15 to v0.0.16 . First we will modify the kube-router DaemonSet resource's image field: kubectl -n kube-system set image ds/kube-router kube-router=cloudnativelabs/kube-router:v0.0.16 This does not actually trigger any version changes yet. It is recommended that you upgrade only one node and perform any tests you see fit to ensure nothing goes wrong. For example, we'll test upgrading kube-router on worker-01: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system delete pod \"${TEST_POD}\" You can watch to make sure the new kube-router pod comes up and stays running with: kubectl -n kube-system get pods -o wide -w Check the logs with: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system logs \"${TEST_POD}\" If it all looks good, go ahead and upgrade kube-router on all nodes: kubectl -n kube-system delete pods -l k8s-app=kube-router","title":"Without Rolling Updates"},{"location":"upgrading/#with-rolling-updates","text":"After updating a DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods will be created automatically, in a controlled fashion If your global BGP peers supports gracefull restarts and has it enabled, rolling updates can be used to upgrade your kube-router DaemonSet without network downtime To enable gracefull BGP restart kube-router must be started with --bgp-graceful-restart To enable rolling updates on your kube-router DaemonSet modify it and add a updateStrategy updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxUnavailable controls the maximum number of pods to simultaneously upgrade Starting from the top of the DaemonSet, it should look like this after you are done editing apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 ...","title":"With Rolling Updates"},{"location":"upgrading/#breaking-change-version-history","text":"This section covers version specific upgrade instructions.","title":"Breaking Change Version History"},{"location":"upgrading/#v00x-alpha-versions","text":"While kube-router is in its alpha stage changes can be expected to be rapid. Therefor we cannot guarantee that a new alpha release will not break previous expected behavior.","title":"v0.0.X alpha versions"},{"location":"upgrading/#v0017-aka-v010-rc1","text":"This version brings changes to hairpin and BGP peering CLI/annotation configuration flags/keys. CLI flag changes: - OLD: --peer-router -> NEW: --peer-router-ips - OLD: --peer-asn -> NEW: --peer-router-asns CLI flag additions: - NEW: --peer-router-passwords Annotation key changes: - OLD: kube-router.io/hairpin-mode= -> NEW: kube-router.io/service.hairpin= - OLD: net.kuberouter.nodeasn= -> NEW: kube-router.io/node.asn= - OLD: net.kuberouter.node.bgppeer.address= -> NEW: kube-router.io/peer.ips - OLD: net.kuberouter.node.bgppeer.asn -> NEW: kube-router.io/peer.asns Annotation key additions: - NEW: kube-router.io/peer.passwords","title":"v0.0.17 (aka v0.1.0-rc1)"},{"location":"upgrading/#v0017-upgrade-procedure","text":"For CLI flag changes, all that is required is to change the flag names you use above to their new names at the same time that you change the image version. kubectl -n kube-system edit ds kube-router For Annotations, the recommended approach is to copy all the values of your current annotations into new annotations with the updated keys. You can get a quick look at all your service and node annotations with these commands: kubectl describe services --all-namespaces |grep -E '^(Name:|Annotations:)' kubectl describe nodes |grep -E '^(Name:|Annotations:)' For example if you have a service annotation to enable Hairpin mode like: Name: hairpin-service Annotations: kube-router.io/hairpin-mode= You will then want to make a new annotation with the new key: kubectl annotate service hairpin-service \"kube-router.io/service.hairpin=\" Once all new annotations are created, proceed with the General Guidelines . After the upgrades tested and complete, you can delete the old annotations. kubectl annotate service hairpin-service \"kube-router.io/hairpin-mode-\"","title":"v0.0.17 Upgrade Procedure"},{"location":"user-guide/","text":"User Guide Try Kube-router with cluster installers The best way to get started is to deploy Kubernetes with Kube-router is with a cluster installer. kops Please see the steps to deploy Kubernetes cluster with Kube-router using Kops bootkube Please see the steps to deploy Kubernetes cluster with Kube-router using bootkube kubeadm Please see the steps to deploy Kubernetes cluster with Kube-router using Kubeadm k0sproject k0s by default uses kube-router as a CNI option. Please see the steps to deploy Kubernetes cluster with Kube-router using k0s generic Please see the steps to deploy kube-router on manually installed clusters Amazon specific notes When running in an AWS environment that requires an explicit proxy you need to inject the proxy server as a environment variable in your kube-router deployment Example: env: - name: HTTP_PROXY value: \"http://proxy.example.com:80\" deployment Depending on what functionality of kube-router you want to use, multiple deployment options are possible. You can use the flags --run-firewall , --run-router , --run-service-proxy to selectively enable only required functionality of kube-router. Also you can choose to run kube-router as agent running on each cluster node. Alternativley you can run kube-router as pod on each node through daemonset. command line options Usage of kube-router: --advertise-cluster-ip Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers. --advertise-external-ip Add External IP of service to the RIB so that it gets advertised to the BGP peers. --advertise-loadbalancer-ip Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers. --advertise-pod-cidr Add Node's POD cidr to the RIB so that it gets advertised to the BGP peers. (default true) --auto-mtu Auto detect and set the largest possible MTU for kube-bridge and pod interfaces (also accounts for IPIP overlay network when enabled). (default true) --bgp-graceful-restart Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts --bgp-graceful-restart-deferral-time duration BGP Graceful restart deferral time according to RFC4724 4.1, maximum 18h. (default 6m0s) --bgp-graceful-restart-time duration BGP Graceful restart time according to RFC4724 3, maximum 4095s. (default 1m30s) --bgp-holdtime duration This parameter is mainly used to modify the holdtime declared to BGP peer. When Kube-router goes down abnormally, the local saving time of BGP route will be affected. Holdtime must be in the range 3s to 18h12m16s. (default 1m30s) --bgp-port uint32 The port open for incoming BGP connections and to use for connecting with other BGP peers. (default 179) --cache-sync-timeout duration The timeout for cache synchronization (e.g. '5s', '1m'). Must be greater than 0. (default 1m0s) --cleanup-config Cleanup iptables rules, ipvs, ipset configuration and exit. --cluster-asn uint ASN number under which cluster nodes will run iBGP. --disable-source-dest-check Disable the source-dest-check attribute for AWS EC2 instances. When this option is false, it must be set some other way. (default true) --enable-cni Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin. (default true) --enable-ibgp Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (default true) --enable-overlay When enable-overlay is set to true, IP-in-IP tunneling is used for pod-to-pod networking across nodes in different subnets. When set to false no tunneling is used and routing infrastructure is expected to route traffic for pod-to-pod networking across nodes in different subnets (default true) --enable-pod-egress SNAT traffic from Pods to destinations outside the cluster. (default true) --enable-pprof Enables pprof for debugging performance and memory leak issues. --excluded-cidrs strings Excluded CIDRs are used to exclude IPVS rules from deletion. --hairpin-mode Add iptables rules for every Service Endpoint to support hairpin traffic. --health-port uint16 Health check port, 0 = Disabled (default 20244) -h, --help Print usage information. --hostname-override string Overrides the NodeName of the node. Set this if kube-router is unable to determine your NodeName automatically. --iptables-sync-period duration The delay between iptables rule synchronizations (e.g. '5s', '1m'). Must be greater than 0. (default 5m0s) --ipvs-graceful-period duration The graceful period before removing destinations from IPVS services (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 30s) --ipvs-graceful-termination Enables the experimental IPVS graceful terminaton capability --ipvs-permit-all Enables rule to accept all incoming traffic to service VIP's on the node. (default true) --ipvs-sync-period duration The delay between ipvs config synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --kubeconfig string Path to kubeconfig file with authorization information (the master location is set by the master flag). --masquerade-all SNAT all traffic to cluster IP/node port. --master string The address of the Kubernetes API server (overrides any value in kubeconfig). --metrics-path string Prometheus metrics path (default \"/metrics\") --metrics-port uint16 Prometheus metrics port, (Default 0, Disabled) --nodeport-bindon-all-ip For service of NodePort type create IPVS service that listens on all IP's of the node. --nodes-full-mesh Each node in the cluster will setup BGP peering with rest of the nodes. (default true) --overlay-type string Possible values: subnet,full - When set to \"subnet\", the default, default \"--enable-overlay=true\" behavior is used. When set to \"full\", it changes \"--enable-overlay=true\" default behavior so that IP-in-IP tunneling is used for pod-to-pod networking across nodes regardless of the subnet the nodes are in. (default \"subnet\") --override-nexthop Override the next-hop in bgp routes sent to peers with the local ip. --peer-router-asns uints ASN numbers of the BGP peer to which cluster nodes will advertise cluster ip and node's pod cidr. (default []) --peer-router-ips ipSlice The ip address of the external router to which all nodes will peer and advertise the cluster ip and pod cidr's. (default []) --peer-router-multihop-ttl uint8 Enable eBGP multihop supports -- sets multihop-ttl. (Relevant only if ttl >= 2) --peer-router-passwords strings Password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords-file string Path to file containing password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords will be preferred if both are set. --peer-router-ports uints The remote port of the external BGP to which all nodes will peer. If not set, default BGP port (179) will be used. (default []) --router-id string BGP router-id. Must be specified in a ipv6 only cluster. --routes-sync-period duration The delay between route updates and advertisements (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --run-firewall Enables Network Policy -- sets up iptables to provide ingress firewall for pods. (default true) --run-router Enables Pod Networking -- Advertises and learns the routes to Pods via iBGP. (default true) --run-service-proxy Enables Service Proxy -- sets up IPVS for Kubernetes Services. (default true) --runtime-endpoint string Path to CRI compatible container runtime socket (used for DSR mode). Currently known working with containerd. --service-cluster-ip-range string CIDR value from which service cluster IPs are assigned. Default: 10.96.0.0/12 (default \"10.96.0.0/12\") --service-external-ip-range strings Specify external IP CIDRs that are used for inter-cluster communication (can be specified multiple times) --service-node-port-range string NodePort range specified with either a hyphen or colon (default \"30000-32767\") -v, --v string log level for V logs (default \"0\") -V, --version Print version information. requirements Kube-router need to access kubernetes API server to get information on pods, services, endpoints, network policies etc. The very minimum information it requires is the details on where to access the kubernetes API server. This information can be passed as kube-router --master=http://192.168.1.99:8080/ or kube-router --kubeconfig=<path to kubeconfig file> . If you run kube-router as agent on the node, ipset package must be installed on each of the nodes (when run as daemonset, container image is prepackaged with ipset) If you choose to use kube-router for pod-to-pod network connectivity then Kubernetes controller manager need to be configured to allocate pod CIDRs by passing --allocate-node-cidrs=true flag and providing a cluster-cidr (i.e. by passing --cluster-cidr=10.1.0.0/16 for e.g.) If you choose to run kube-router as daemonset in Kubernetes version below v1.15, both kube-apiserver and kubelet must be run with --allow-privileged=true option. In later Kubernetes versions, only kube-apiserver must be run with --allow-privileged=true option and if PodSecurityPolicy admission controller is enabled, you should create PodSecurityPolicy, allowing privileged kube-router pods. If you choose to use kube-router for pod-to-pod network connecitvity then Kubernetes cluster must be configured to use CNI network plugins. On each node CNI conf file is expected to be present as /etc/cni/net.d/10-kuberouter.conf . bridge CNI plugin and host-local for IPAM should be used. A sample conf file that can be downloaded as wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf running as daemonset This is quickest way to deploy kube-router in Kubernetes v1.8+ ( dont forget to ensure the requirements ). Just run kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml Above will run kube-router as pod on each node automatically. You can change the arguments in the daemonset definition as required to suit your needs. Some samples can be found at https://github.com/cloudnativelabs/kube-router/tree/master/daemonset with different argument to select set of the services kube-router should run. running as agent You can choose to run kube-router as agent runnng on each node. For e.g if you just want kube-router to provide ingress firewall for the pods then you can start kube-router as kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=false cleanup configuration Please delete kube-router daemonset and then clean up all the configurations done (to ipvs, iptables, ipset, ip routes etc) by kube-router on the node by running below command. docker run --privileged --net=host cloudnativelabs/kube-router --cleanup-config trying kube-router as alternative to kube-proxy If you have a kube-proxy in use, and want to try kube-router just for service proxy you can do kube-proxy --cleanup-iptables followed by kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false and if you want to move back to kube-proxy then clean up config done by kube-router by running kube-router --cleanup-config and run kube-proxy with the configuration you have. - General Setup Advertising IPs kube-router can advertise Cluster, External and LoadBalancer IPs to BGP peers. It does this by: * locally adding the advertised IPs to the nodes' kube-dummy-if network interface * advertising the IPs to its BGP peers To set the default for all services use the --advertise-cluster-ip , --advertise-external-ip and --advertise-loadbalancer-ip flags. To selectively enable or disable this feature per-service use the kube-router.io/service.advertise.clusterip , kube-router.io/service.advertise.externalip and kube-router.io/service.advertise.loadbalancerip annotations. e.g.: $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.clusterip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.externalip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.loadbalancerip=true\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.clusterip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.externalip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.loadbalancerip=false\" By combining the flags with the per-service annotations you can choose either a opt-in or opt-out strategy for advertising IPs. Advertising LoadBalancer IPs works by inspecting the services status.loadBalancer.ingress IPs that are set by external LoadBalancers like for example MetalLb. This has been successfully tested together with MetalLB in ARP mode. Hairpin Mode Communication from a Pod that is behind a Service to its own ClusterIP:Port is not supported by default. However, It can be enabled per-service by adding the kube-router.io/service.hairpin= annotation, or for all Services in a cluster by passing the flag --hairpin-mode=true to kube-router. Additionally, the hairpin_mode sysctl option must be set to 1 for all veth interfaces on each node. This can be done by adding the \"hairpinMode\": true option to your CNI configuration and rebooting all cluster nodes if they are already running kubernetes. Hairpin traffic will be seen by the pod it originated from as coming from the Service ClusterIP if it is logging the source IP. Hairpin Mode Example 10-kuberouter.conf { \"name\":\"mynet\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"hairpinMode\":true, \"ipam\": { \"type\":\"host-local\" } } To enable hairpin traffic for Service my-service : kubectl annotate service my-service \"kube-router.io/service.hairpin=\" If you want to also hairpin externalIPs declared for Service my-service (note, you must also either enable global hairpin or service hairpin (see above ^^^) for this to have an effect): kubectl annotate service my-service \"kube-router.io/service.hairpin.externalips=\" SNATing Service Traffic By default, as traffic ingresses into the cluster, kube-router will source nat the traffic to ensure symmetric routing if it needs to proxy that traffic to ensure it gets to a node that has a service pod that is capable of servicing the traffic. This has a potential to cause issues when network policies are applied to that service since now the traffic will appear to be coming from a node in your cluster instead of the traffic originator. This is an issue that is common to all proxy's and all Kubernetes service proxies in general. You can read more information about this issue here: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-nodeport In addition to the fix mentioned in the linked upstream documentation (using service.spec.externalTrafficPolicy ), kube-router also provides DSR, which by its nature preserves the source IP, to solve this problem. For more information see the section above. Load balancing Scheduling Algorithms Kube-router uses LVS for service proxy. LVS support rich set of scheduling alogirthms . You can annotate the service to choose one of the scheduling alogirthms. When a service is not annotated round-robin scheduler is selected by default For least connection scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=lc\" For round-robin scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=rr\" For source hashing scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=sh\" For destination hashing scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=dh\" HostPort support If you would like to use HostPort functionality below changes are required in the manifest. By default kube-router assumes CNI conf file to be /etc/cni/net.d/10-kuberouter.conf . Add an environment variable KUBE_ROUTER_CNI_CONF_FILE to kube-router manifest and set it to /etc/cni/net.d/10-kuberouter.conflist Modify kube-router-cfg ConfigMap with CNI config that supports portmap as additional plug-in { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } }, { \"type\":\"portmap\", \"capabilities\":{ \"snat\":true, \"portMappings\":true } } ] } Update init container command to create /etc/cni/net.d/10-kuberouter.conflist file Restart the container runtime For an e.g manifest please look at manifest with necessary changes required for HostPort functionality. IPVS Graceful termination support As of 0.2.6 we support experimental graceful termination of IPVS destinations. When possible the pods's TerminationGracePeriodSeconds is used, if it cannot be retrived for some reason the fallback period is 30 seconds and can be adjusted with --ipvs-graceful-period cli-opt graceful termination works in such a way that when kube-router receives a delete endpoint notification for a service it's weight is adjusted to 0 before getting deleted after he termination grace period has passed or the Active & Inactive connections goes down to 0. MTU The maximum transmission unit (MTU) determines the largest packet size that can be transmitted through your network. MTU for the pod interfaces should be set appropriately to prevent fragmentation and packet drops thereby achieving maximum performance. If auto-mtu is set to true ( auto-mtu is set to true by default as of kube-router 1.1), kube-router will determine right MTU for both kube-bridge and pod interfaces. If you set auto-mtu to false kube-router will not attempt to configure MTU. However you can choose the right MTU and set in the cni-conf.json section of the 10-kuberouter.conflist in the kube-router daemonsets . For e.g. cni-conf.json: | { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"mtu\": 1400, \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } } ] } If you set MTU yourself via the CNI config, you'll also need to set MTU of kube-bridge manually to the right value to avoid packet fragmentation in case of existing nodes on which kube-bridge is already created. On node reboot or in case of new nodes joining the cluster both the pod's interface and kube-bridge will be setup with specified MTU value. BGP configuration Configuring BGP Peers Metrics Configure metrics gathering","title":"User Guide"},{"location":"user-guide/#user-guide","text":"","title":"User Guide"},{"location":"user-guide/#try-kube-router-with-cluster-installers","text":"The best way to get started is to deploy Kubernetes with Kube-router is with a cluster installer.","title":"Try Kube-router with cluster installers"},{"location":"user-guide/#kops","text":"Please see the steps to deploy Kubernetes cluster with Kube-router using Kops","title":"kops"},{"location":"user-guide/#bootkube","text":"Please see the steps to deploy Kubernetes cluster with Kube-router using bootkube","title":"bootkube"},{"location":"user-guide/#kubeadm","text":"Please see the steps to deploy Kubernetes cluster with Kube-router using Kubeadm","title":"kubeadm"},{"location":"user-guide/#k0sproject","text":"k0s by default uses kube-router as a CNI option. Please see the steps to deploy Kubernetes cluster with Kube-router using k0s","title":"k0sproject"},{"location":"user-guide/#generic","text":"Please see the steps to deploy kube-router on manually installed clusters","title":"generic"},{"location":"user-guide/#amazon-specific-notes","text":"When running in an AWS environment that requires an explicit proxy you need to inject the proxy server as a environment variable in your kube-router deployment Example: env: - name: HTTP_PROXY value: \"http://proxy.example.com:80\"","title":"Amazon specific notes"},{"location":"user-guide/#deployment","text":"Depending on what functionality of kube-router you want to use, multiple deployment options are possible. You can use the flags --run-firewall , --run-router , --run-service-proxy to selectively enable only required functionality of kube-router. Also you can choose to run kube-router as agent running on each cluster node. Alternativley you can run kube-router as pod on each node through daemonset.","title":"deployment"},{"location":"user-guide/#command-line-options","text":"Usage of kube-router: --advertise-cluster-ip Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers. --advertise-external-ip Add External IP of service to the RIB so that it gets advertised to the BGP peers. --advertise-loadbalancer-ip Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers. --advertise-pod-cidr Add Node's POD cidr to the RIB so that it gets advertised to the BGP peers. (default true) --auto-mtu Auto detect and set the largest possible MTU for kube-bridge and pod interfaces (also accounts for IPIP overlay network when enabled). (default true) --bgp-graceful-restart Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts --bgp-graceful-restart-deferral-time duration BGP Graceful restart deferral time according to RFC4724 4.1, maximum 18h. (default 6m0s) --bgp-graceful-restart-time duration BGP Graceful restart time according to RFC4724 3, maximum 4095s. (default 1m30s) --bgp-holdtime duration This parameter is mainly used to modify the holdtime declared to BGP peer. When Kube-router goes down abnormally, the local saving time of BGP route will be affected. Holdtime must be in the range 3s to 18h12m16s. (default 1m30s) --bgp-port uint32 The port open for incoming BGP connections and to use for connecting with other BGP peers. (default 179) --cache-sync-timeout duration The timeout for cache synchronization (e.g. '5s', '1m'). Must be greater than 0. (default 1m0s) --cleanup-config Cleanup iptables rules, ipvs, ipset configuration and exit. --cluster-asn uint ASN number under which cluster nodes will run iBGP. --disable-source-dest-check Disable the source-dest-check attribute for AWS EC2 instances. When this option is false, it must be set some other way. (default true) --enable-cni Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin. (default true) --enable-ibgp Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (default true) --enable-overlay When enable-overlay is set to true, IP-in-IP tunneling is used for pod-to-pod networking across nodes in different subnets. When set to false no tunneling is used and routing infrastructure is expected to route traffic for pod-to-pod networking across nodes in different subnets (default true) --enable-pod-egress SNAT traffic from Pods to destinations outside the cluster. (default true) --enable-pprof Enables pprof for debugging performance and memory leak issues. --excluded-cidrs strings Excluded CIDRs are used to exclude IPVS rules from deletion. --hairpin-mode Add iptables rules for every Service Endpoint to support hairpin traffic. --health-port uint16 Health check port, 0 = Disabled (default 20244) -h, --help Print usage information. --hostname-override string Overrides the NodeName of the node. Set this if kube-router is unable to determine your NodeName automatically. --iptables-sync-period duration The delay between iptables rule synchronizations (e.g. '5s', '1m'). Must be greater than 0. (default 5m0s) --ipvs-graceful-period duration The graceful period before removing destinations from IPVS services (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 30s) --ipvs-graceful-termination Enables the experimental IPVS graceful terminaton capability --ipvs-permit-all Enables rule to accept all incoming traffic to service VIP's on the node. (default true) --ipvs-sync-period duration The delay between ipvs config synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --kubeconfig string Path to kubeconfig file with authorization information (the master location is set by the master flag). --masquerade-all SNAT all traffic to cluster IP/node port. --master string The address of the Kubernetes API server (overrides any value in kubeconfig). --metrics-path string Prometheus metrics path (default \"/metrics\") --metrics-port uint16 Prometheus metrics port, (Default 0, Disabled) --nodeport-bindon-all-ip For service of NodePort type create IPVS service that listens on all IP's of the node. --nodes-full-mesh Each node in the cluster will setup BGP peering with rest of the nodes. (default true) --overlay-type string Possible values: subnet,full - When set to \"subnet\", the default, default \"--enable-overlay=true\" behavior is used. When set to \"full\", it changes \"--enable-overlay=true\" default behavior so that IP-in-IP tunneling is used for pod-to-pod networking across nodes regardless of the subnet the nodes are in. (default \"subnet\") --override-nexthop Override the next-hop in bgp routes sent to peers with the local ip. --peer-router-asns uints ASN numbers of the BGP peer to which cluster nodes will advertise cluster ip and node's pod cidr. (default []) --peer-router-ips ipSlice The ip address of the external router to which all nodes will peer and advertise the cluster ip and pod cidr's. (default []) --peer-router-multihop-ttl uint8 Enable eBGP multihop supports -- sets multihop-ttl. (Relevant only if ttl >= 2) --peer-router-passwords strings Password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords-file string Path to file containing password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords will be preferred if both are set. --peer-router-ports uints The remote port of the external BGP to which all nodes will peer. If not set, default BGP port (179) will be used. (default []) --router-id string BGP router-id. Must be specified in a ipv6 only cluster. --routes-sync-period duration The delay between route updates and advertisements (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --run-firewall Enables Network Policy -- sets up iptables to provide ingress firewall for pods. (default true) --run-router Enables Pod Networking -- Advertises and learns the routes to Pods via iBGP. (default true) --run-service-proxy Enables Service Proxy -- sets up IPVS for Kubernetes Services. (default true) --runtime-endpoint string Path to CRI compatible container runtime socket (used for DSR mode). Currently known working with containerd. --service-cluster-ip-range string CIDR value from which service cluster IPs are assigned. Default: 10.96.0.0/12 (default \"10.96.0.0/12\") --service-external-ip-range strings Specify external IP CIDRs that are used for inter-cluster communication (can be specified multiple times) --service-node-port-range string NodePort range specified with either a hyphen or colon (default \"30000-32767\") -v, --v string log level for V logs (default \"0\") -V, --version Print version information.","title":"command line options"},{"location":"user-guide/#requirements","text":"Kube-router need to access kubernetes API server to get information on pods, services, endpoints, network policies etc. The very minimum information it requires is the details on where to access the kubernetes API server. This information can be passed as kube-router --master=http://192.168.1.99:8080/ or kube-router --kubeconfig=<path to kubeconfig file> . If you run kube-router as agent on the node, ipset package must be installed on each of the nodes (when run as daemonset, container image is prepackaged with ipset) If you choose to use kube-router for pod-to-pod network connectivity then Kubernetes controller manager need to be configured to allocate pod CIDRs by passing --allocate-node-cidrs=true flag and providing a cluster-cidr (i.e. by passing --cluster-cidr=10.1.0.0/16 for e.g.) If you choose to run kube-router as daemonset in Kubernetes version below v1.15, both kube-apiserver and kubelet must be run with --allow-privileged=true option. In later Kubernetes versions, only kube-apiserver must be run with --allow-privileged=true option and if PodSecurityPolicy admission controller is enabled, you should create PodSecurityPolicy, allowing privileged kube-router pods. If you choose to use kube-router for pod-to-pod network connecitvity then Kubernetes cluster must be configured to use CNI network plugins. On each node CNI conf file is expected to be present as /etc/cni/net.d/10-kuberouter.conf . bridge CNI plugin and host-local for IPAM should be used. A sample conf file that can be downloaded as wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf","title":"requirements"},{"location":"user-guide/#running-as-daemonset","text":"This is quickest way to deploy kube-router in Kubernetes v1.8+ ( dont forget to ensure the requirements ). Just run kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml Above will run kube-router as pod on each node automatically. You can change the arguments in the daemonset definition as required to suit your needs. Some samples can be found at https://github.com/cloudnativelabs/kube-router/tree/master/daemonset with different argument to select set of the services kube-router should run.","title":"running as daemonset"},{"location":"user-guide/#running-as-agent","text":"You can choose to run kube-router as agent runnng on each node. For e.g if you just want kube-router to provide ingress firewall for the pods then you can start kube-router as kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=false","title":"running as agent"},{"location":"user-guide/#cleanup-configuration","text":"Please delete kube-router daemonset and then clean up all the configurations done (to ipvs, iptables, ipset, ip routes etc) by kube-router on the node by running below command. docker run --privileged --net=host cloudnativelabs/kube-router --cleanup-config","title":"cleanup configuration"},{"location":"user-guide/#trying-kube-router-as-alternative-to-kube-proxy","text":"If you have a kube-proxy in use, and want to try kube-router just for service proxy you can do kube-proxy --cleanup-iptables followed by kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false and if you want to move back to kube-proxy then clean up config done by kube-router by running kube-router --cleanup-config and run kube-proxy with the configuration you have. - General Setup","title":"trying kube-router as alternative to kube-proxy"},{"location":"user-guide/#advertising-ips","text":"kube-router can advertise Cluster, External and LoadBalancer IPs to BGP peers. It does this by: * locally adding the advertised IPs to the nodes' kube-dummy-if network interface * advertising the IPs to its BGP peers To set the default for all services use the --advertise-cluster-ip , --advertise-external-ip and --advertise-loadbalancer-ip flags. To selectively enable or disable this feature per-service use the kube-router.io/service.advertise.clusterip , kube-router.io/service.advertise.externalip and kube-router.io/service.advertise.loadbalancerip annotations. e.g.: $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.clusterip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.externalip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.loadbalancerip=true\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.clusterip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.externalip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.loadbalancerip=false\" By combining the flags with the per-service annotations you can choose either a opt-in or opt-out strategy for advertising IPs. Advertising LoadBalancer IPs works by inspecting the services status.loadBalancer.ingress IPs that are set by external LoadBalancers like for example MetalLb. This has been successfully tested together with MetalLB in ARP mode.","title":"Advertising IPs"},{"location":"user-guide/#hairpin-mode","text":"Communication from a Pod that is behind a Service to its own ClusterIP:Port is not supported by default. However, It can be enabled per-service by adding the kube-router.io/service.hairpin= annotation, or for all Services in a cluster by passing the flag --hairpin-mode=true to kube-router. Additionally, the hairpin_mode sysctl option must be set to 1 for all veth interfaces on each node. This can be done by adding the \"hairpinMode\": true option to your CNI configuration and rebooting all cluster nodes if they are already running kubernetes. Hairpin traffic will be seen by the pod it originated from as coming from the Service ClusterIP if it is logging the source IP.","title":"Hairpin Mode"},{"location":"user-guide/#hairpin-mode-example","text":"10-kuberouter.conf { \"name\":\"mynet\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"hairpinMode\":true, \"ipam\": { \"type\":\"host-local\" } } To enable hairpin traffic for Service my-service : kubectl annotate service my-service \"kube-router.io/service.hairpin=\" If you want to also hairpin externalIPs declared for Service my-service (note, you must also either enable global hairpin or service hairpin (see above ^^^) for this to have an effect): kubectl annotate service my-service \"kube-router.io/service.hairpin.externalips=\"","title":"Hairpin Mode Example"},{"location":"user-guide/#snating-service-traffic","text":"By default, as traffic ingresses into the cluster, kube-router will source nat the traffic to ensure symmetric routing if it needs to proxy that traffic to ensure it gets to a node that has a service pod that is capable of servicing the traffic. This has a potential to cause issues when network policies are applied to that service since now the traffic will appear to be coming from a node in your cluster instead of the traffic originator. This is an issue that is common to all proxy's and all Kubernetes service proxies in general. You can read more information about this issue here: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-nodeport In addition to the fix mentioned in the linked upstream documentation (using service.spec.externalTrafficPolicy ), kube-router also provides DSR, which by its nature preserves the source IP, to solve this problem. For more information see the section above.","title":"SNATing Service Traffic"},{"location":"user-guide/#load-balancing-scheduling-algorithms","text":"Kube-router uses LVS for service proxy. LVS support rich set of scheduling alogirthms . You can annotate the service to choose one of the scheduling alogirthms. When a service is not annotated round-robin scheduler is selected by default For least connection scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=lc\" For round-robin scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=rr\" For source hashing scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=sh\" For destination hashing scheduling use: kubectl annotate service my-service \"kube-router.io/service.scheduler=dh\"","title":"Load balancing Scheduling Algorithms"},{"location":"user-guide/#hostport-support","text":"If you would like to use HostPort functionality below changes are required in the manifest. By default kube-router assumes CNI conf file to be /etc/cni/net.d/10-kuberouter.conf . Add an environment variable KUBE_ROUTER_CNI_CONF_FILE to kube-router manifest and set it to /etc/cni/net.d/10-kuberouter.conflist Modify kube-router-cfg ConfigMap with CNI config that supports portmap as additional plug-in { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } }, { \"type\":\"portmap\", \"capabilities\":{ \"snat\":true, \"portMappings\":true } } ] } Update init container command to create /etc/cni/net.d/10-kuberouter.conflist file Restart the container runtime For an e.g manifest please look at manifest with necessary changes required for HostPort functionality.","title":"HostPort support"},{"location":"user-guide/#ipvs-graceful-termination-support","text":"As of 0.2.6 we support experimental graceful termination of IPVS destinations. When possible the pods's TerminationGracePeriodSeconds is used, if it cannot be retrived for some reason the fallback period is 30 seconds and can be adjusted with --ipvs-graceful-period cli-opt graceful termination works in such a way that when kube-router receives a delete endpoint notification for a service it's weight is adjusted to 0 before getting deleted after he termination grace period has passed or the Active & Inactive connections goes down to 0.","title":"IPVS Graceful termination support"},{"location":"user-guide/#mtu","text":"The maximum transmission unit (MTU) determines the largest packet size that can be transmitted through your network. MTU for the pod interfaces should be set appropriately to prevent fragmentation and packet drops thereby achieving maximum performance. If auto-mtu is set to true ( auto-mtu is set to true by default as of kube-router 1.1), kube-router will determine right MTU for both kube-bridge and pod interfaces. If you set auto-mtu to false kube-router will not attempt to configure MTU. However you can choose the right MTU and set in the cni-conf.json section of the 10-kuberouter.conflist in the kube-router daemonsets . For e.g. cni-conf.json: | { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"mtu\": 1400, \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } } ] } If you set MTU yourself via the CNI config, you'll also need to set MTU of kube-bridge manually to the right value to avoid packet fragmentation in case of existing nodes on which kube-bridge is already created. On node reboot or in case of new nodes joining the cluster both the pod's interface and kube-bridge will be setup with specified MTU value.","title":"MTU"},{"location":"user-guide/#bgp-configuration","text":"Configuring BGP Peers","title":"BGP configuration"},{"location":"user-guide/#metrics","text":"Configure metrics gathering","title":"Metrics"}]}